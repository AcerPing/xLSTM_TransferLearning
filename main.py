import random
import argparse # è§£æå‘½ä»¤åˆ—åƒæ•¸
import json
import os
from os import path, getcwd, makedirs, environ, listdir
import shutil
import numpy as np

import tensorflow as tf
import keras
from sklearn.model_selection import train_test_split
from keras.models import load_model
from keras.callbacks import CSVLogger, ModelCheckpoint, ReduceLROnPlateau, EarlyStopping 

    # from recombinator.optimal_block_length import optimal_block_length # æ ¹æ“šæ•¸æ“šçš„ç‰¹æ€§ï¼Œè¨ˆç®—åºåˆ—æ•¸æ“šçš„æœ€ä½³å€å¡Šé•·åº¦ï¼ˆblock lengthï¼‰ã€‚
    # â†‘ å€å¡Šé•·åº¦çš„æ„ç¾©ï¼šå€å¡Šé•·åº¦è¶Šé•·ï¼Œæ•¸æ“šçš„æ™‚é–“ä¾è³´æ€§è¢«ä¿ç•™å¾—è¶Šå¤šï¼Œä½†éš¨æ©Ÿæ€§æ¸›å°‘ï¼›å€å¡Šé•·åº¦è¶ŠçŸ­ï¼Œæ•¸æ“šæ›´å…·éš¨æ©Ÿæ€§ï¼Œä½†å¯èƒ½å¤±å»æ™‚é–“ä¾è³´ä¿¡æ¯ã€‚
    # from recombinator.block_bootstrap import circular_block_bootstrap # ç”¨æ–¼å°å…·æœ‰æ™‚é–“ä¾è³´æ€§çš„æ•¸æ“šé€²è¡Œé‡æŠ½æ¨£ï¼Œåœ¨ä¸æ‰“ç ´æ•¸æ“šæ™‚é–“ä¾è³´æ€§çš„æƒ…æ³ä¸‹ç”Ÿæˆæ–°çš„æ•¸æ“šã€‚æ¡ç”¨çš„æ˜¯å¾ªç’°æŠ½æ¨£çš„æ–¹å¼ï¼Œé€™æ„å‘³è‘—ç•¶æŠ½æ¨£åˆ°åºåˆ—å°¾ç«¯æ™‚ï¼Œå¯ä»¥å›åˆ°åºåˆ—é–‹é ­ç¹¼çºŒæŠ½æ¨£ã€‚

import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import TensorDataset, DataLoader
import torch.utils.cpp_extension

# å–å¾—åŸå§‹ include_paths()ï¼Œé¿å…éè¿´å‘¼å«è‡ªå·±
_original_include_paths = torch.utils.cpp_extension.include_paths

def include_paths_patched(*args, **kwargs):
    if 'cuda' in kwargs:
        del kwargs['cuda']  # ç§»é™¤ä¸æ”¯æ´çš„åƒæ•¸
    return _original_include_paths(*args, **kwargs)

torch.utils.cpp_extension.include_paths = include_paths_patched # é€²è¡Œ Monkey Patch
print("âœ… Patched torch.utils.cpp_extension.include_paths successfully!")

import xlstm  # é€™è¡Œä¸€å®šè¦æ”¾åœ¨ Monkey Patch ä¹‹å¾Œ

from utils.model import build_model, rmse, train_model
from utils.data_io import read_data_from_dataset
from utils.save import save_lr_curve, save_prediction_plot, save_yy_plot, save_mse, ResidualPlot, ErrorHistogram
from utils.device import limit_gpu_memory # é™åˆ¶ TensorFlow å° GPU è¨˜æ†¶é«”çš„é ç•™æˆ–ä½¿ç”¨é‡ã€‚
from Ensemble import start_ensemble # æ•´é«”å­¸ç¿’
from reports.Record_args_while_training import Record_args_while_training # ç´€éŒ„è¨“ç·´æ™‚çš„nb_batchã€bsizeã€period
from reports.Metrics_Comparison import metrics_comparison # æ¯”è¼ƒ Transfer-Learningé·ç§»å­¸ç¿’ vs. Without-Transfer-Learningä¸ä½¿ç”¨é·ç§»å­¸ç¿’
from reports.output import MSE_Improvement, MAE_Improvement # æ¯”è¼ƒ Transfer-Learningé·ç§»å­¸ç¿’ vs. Without-Transfer-Learningä¸ä½¿ç”¨é·ç§»å­¸ç¿’
from reports.util import dataset_idx_vs_improvement # æ¯”è¼ƒç‰¹å¾µéç›¸ä¼¼ç¨‹åº¦èˆ‡MSEã€MAEæ”¹é€²ç¨‹åº¦


def parse_arguments():
    ap = argparse.ArgumentParser(
        description='Time-Series Regression by LSTM through transfer learning') # è¡¨ç¤ºè©²ç¨‹å¼çš„ç”¨é€”æ˜¯é€éé·ç§»å­¸ç¿’ä½¿ç”¨ LSTM é€²è¡Œæ™‚é–“åºåˆ—å›æ­¸åˆ†æã€‚
    
    # for dataset path
    ap.add_argument('--out-dir', '-o', default='result',
                    type=str, help='path for output directory') # æŒ‡å®šè¼¸å‡ºç›®éŒ„çš„è·¯å¾‘ï¼Œé è¨­å€¼ç‚º resultã€‚
    
    # for model
    ap.add_argument('--seed', type=int, default=1234,
                    help='seed value for random value, (default : 1234)') # ç¢ºä¿éš¨æ©Ÿæ“ä½œï¼ˆå¦‚è³‡æ–™åˆ†å‰²ã€æ¨¡å‹åˆå§‹åŒ–ç­‰ï¼‰åœ¨æ¯æ¬¡åŸ·è¡Œä¸­ä¸€è‡´ï¼Œæ–¹ä¾¿å¯¦é©—é‡ç¾æ€§ã€‚
    ap.add_argument('--train-ratio', default=0.8, type=float,
                    help='percentage of train data to be loaded (default : 0.8)') # æŒ‡å®šè¨“ç·´é›†æ¯”ä¾‹ç‚º 0.8ï¼ˆå³ 80%ï¼‰ã€‚æ•¸æ“šé›†æœƒä¾æ“šæ­¤æ¯”ä¾‹åˆ†å‰²ç‚ºè¨“ç·´é›†å’Œæ¸¬è©¦é›†æˆ–é©—è­‰é›†ã€‚
    ap.add_argument('--time-window', default=1000, type=int,
                    help='length of time to capture at once (default : 1000)') # è¨­å®šæ™‚é–“çª—å£ç‚º 1000ã€‚é€™å¯èƒ½ä»£è¡¨æ¨¡å‹åœ¨ä¸€æ¬¡è™•ç†éç¨‹ä¸­è§€å¯Ÿçš„è³‡æ–™é•·åº¦æˆ–æ™‚é–“ç¯„åœã€‚
    
    # for training
    ap.add_argument('--train-mode', '-m', default='pre-train', type=str,
                    help='"pre-train", "transfer-learning", "without-transfer-learning", "comparison"\
                            "ensemble", "bagging", "noise-injection", "score" (default : pre-train)') # è¨­å®šæ¨¡å¼
    ap.add_argument('--gpu', action='store_true',
                    help='whether to do calculations on gpu machines (default : False)') # æ˜¯å¦å•Ÿç”¨GPUåŠ é€Ÿ
    ap.add_argument('--nb-epochs', '-e', default=1, type=int,
                    help='training epochs for the model (default : 1)') # è¨­å®šè¨“ç·´çš„epochã€‚ï¼ˆepochæ˜¯å®Œæ•´åœ°ä½¿ç”¨æ‰€æœ‰è¨“ç·´æ•¸æ“šè¨“ç·´æ¨¡å‹çš„ä¸€æ¬¡éç¨‹ã€‚ï¼‰
    ap.add_argument('--nb-batch', default=20, type=int,
                    help='number of batches in training (default : 20)') # è¨­å®šè¨“ç·´éç¨‹ä¸­çš„æ‰¹æ¬¡æ•¸é‡ï¼Œé è¨­ç‚º 20ã€‚ æ‰¹æ¬¡å¤§å°ï¼ˆbatch sizeï¼‰ = ç¸½è¨“ç·´æ¨£æœ¬æ•¸é‡ Ã· æ‰¹æ¬¡æ•¸é‡ï¼ˆnb-batchï¼‰
    # ap.add_argument('--nb-subset', default=10, type=int,
    #                 help='number of data subset in bootstrapping (default : 10)') # åœ¨bootstrappingä¸­(å³Baggingé›†æˆå¼å­¸ç¿’)è¨­å®šè³‡æ–™å­é›†çš„æ•¸é‡ã€‚EX. ç”Ÿæˆ 10 å€‹ä¸åŒçš„è¨“ç·´å­é›†ã€‚
    ap.add_argument('--noise-var', default=0.0001, type=float,
                    help='variance of noise in noise injection (default : 0.0001)') # åœ¨å™ªè²æ³¨å…¥ä¸­è¨­å®šå™ªè²çš„è®Šç•°æ•¸ã€‚
    ap.add_argument('--valid-ratio', default=0.2, type=float,
                    help='ratio of validation data in train data (default : 0.2)') # åœ¨è¨“ç·´è³‡æ–™ä¸­è¨­å®šé©—è­‰è³‡æ–™çš„æ¯”ä¾‹ã€‚
    ap.add_argument('--freeze', action='store_true', 
                    help='whether to freeze transferred weights in transfer learning (default : False)') # åœ¨é·ç§»å­¸ç¿’ä¸­å‡çµå·²è½‰ç§»çš„æ¬Šé‡ã€‚

    # for output
    ap.add_argument('--train-verbose', default=1, type=int,
                    help='whether to show the learning process (default : 1)') # è¨­å®šè¨“ç·´éç¨‹ä¸­çš„è¼¸å‡ºè©³ç›¡ç¨‹åº¦ã€‚
    args = vars(ap.parse_args())
    return args
    

def seed_every_thing(seed=1234): # ç¢ºä¿å„ç¨®éš¨æ©Ÿæ“ä½œï¼ˆå¦‚è³‡æ–™åˆ†å‰²ã€æ¨¡å‹åˆå§‹åŒ–ç­‰ï¼‰åœ¨æ¯æ¬¡åŸ·è¡Œä¸­ç”¢ç”Ÿç›¸åŒçš„çµæœï¼Œå¾è€Œæé«˜å¯¦é©—çš„å¯é‡ç¾æ€§ã€‚
    environ['PYTHONHASHSEED'] = str(seed) # è¨­å®šPythonçš„é›œæ¹Šéš¨æ©Ÿç¨®å­ï¼Œç¢ºä¿Pythonçš„é›œæ¹Šè¡Œç‚ºåœ¨æ¯æ¬¡åŸ·è¡Œæ™‚ä¿æŒä¸€è‡´ã€‚
    np.random.seed(seed) # è¨­å®šNumPyçš„éš¨æ©Ÿç¨®å­ï¼Œç¢ºä¿ NumPy ç”¢ç”Ÿçš„éš¨æ©Ÿæ•¸åœ¨æ¯æ¬¡åŸ·è¡Œæ™‚ç›¸åŒã€‚
    random.seed(seed) # è¨­å®šPythonæ¨™æº–åº«çš„éš¨æ©Ÿç¨®å­ï¼Œç¢ºä¿Pythonæ¨™æº–åº«ä¸­çš„éš¨æ©Ÿæ•¸ç”Ÿæˆå™¨åœ¨æ¯æ¬¡åŸ·è¡Œæ™‚ç”¢ç”Ÿç›¸åŒçš„çµæœã€‚
    tf.random.set_seed(seed)  # TensorFlow 2.x è¨­å®šéš¨æ©Ÿç¨®å­çš„æ­£ç¢ºæ–¹å¼ï¼Œç¢ºä¿TensorFlowç”¢ç”Ÿçš„éš¨æ©Ÿæ•¸åœ¨æ¯æ¬¡åŸ·è¡Œæ™‚ä¸€è‡´ã€‚


def save_arguments(args, out_dir): # æ—¨åœ¨å°‡åƒæ•¸å­—å…¸ args ä»¥ JSON æ ¼å¼ä¿å­˜åˆ°æŒ‡å®šçš„è¼¸å‡ºç›®éŒ„ out_dir ä¸­
    path_arguments = path.join(out_dir, 'params.json')
    with open(path_arguments, mode="w") as f:
        json.dump(args, f, indent=4)


# def make_callbacks(file_path, save_csv=True):
#     # reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.5, verbose=1, patience=4, min_lr=1e-7) # é™ä½å­¸ç¿’ç‡ï¼Œä»¥ä¿ƒé€²æ¨¡å‹æ›´å¥½åœ°æ”¶æ–‚ã€‚
#     model_checkpoint = ModelCheckpoint(filepath=file_path, monitor='val_loss', save_best_only=True) # ä¿å­˜æœ€ä½³æ¨¡å‹ã€‚ # -- save_weights_only = True,
#     # early_stopping = EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True) 
#     if not save_csv:
#         return [reduce_lr, model_checkpoint, early_stopping]
#     csv_logger = CSVLogger(path.join(path.dirname(file_path), 'epoch_log.csv')) # å°‡æ¯å€‹è¨“ç·´é€±æœŸçš„æå¤±å’Œè©•ä¼°æŒ‡æ¨™è¨˜éŒ„åˆ° CSV æ–‡ä»¶ä¸­
#     return [reduce_lr, model_checkpoint, csv_logger, early_stopping] 


def create_sliding_window(X, y, sequence_length):
    """
    ä½¿ç”¨æ»‘å‹•çª—å£æ–¹æ³•ï¼Œå°‡æ™‚é–“åºåˆ—è½‰æ›ç‚º (batch_size, sequence_length, features) æ ¼å¼ã€‚
    å°‡åŸå§‹çš„æ™‚é–“åºåˆ—è³‡æ–™ (X) åˆ‡æˆã€Œä¸€å°æ®µä¸€å°æ®µçš„åºåˆ—ç‰‡æ®µã€çµ¦ LSTM ä½¿ç”¨ï¼Œå› ç‚º LSTM è¦åƒçš„æ˜¯ (batch_size, sequence_length, features) æ ¼å¼çš„è³‡æ–™ã€‚
    :param X: åŸå§‹ç‰¹å¾µæ•¸æ“š (samples, features)
    :param y: æ¨™ç±¤æ•¸æ“š (samples,)
    :param sequence_length: LSTM æ‰€éœ€çš„æ™‚é–“æ­¥é•·
    sequence_length=1440 ä»£è¡¨æˆ‘è¦ç”¨ã€Œå‰ 1440 ç­†è³‡æ–™ã€å»é æ¸¬ã€Œç¬¬ 1441 ç­†çš„å€¼ã€ã€‚
    :return: æ»‘å‹•çª—å£æ ¼å¼çš„ X å’Œ y
    """
    X_seq, y_seq = [], []
    total_samples = len(X)
    for i in range(total_samples - sequence_length): # for è¿´åœˆåˆ‡å‡ºæ¯ä¸€çµ„åºåˆ—æ¨£æœ¬
        X_seq.append(X[i: i + sequence_length])  # å– sequence_length é•·åº¦çš„å€é–“
        y_seq.append(y[i + sequence_length])  # é æ¸¬ sequence_length ä¹‹å¾Œçš„å€¼
    return np.array(X_seq), np.array(y_seq)
 

def main():

    # make analysis environment
    limit_gpu_memory() # é™åˆ¶GPUè¨˜æ†¶é«”ä½¿ç”¨é‡ï¼Œé¿å…å› ç‚ºåˆ†é…éå¤šè€Œé€ æˆç³»çµ±ä¸ç©©å®šã€‚ç„¶è€Œç•¶ä½¿ç”¨é‡è¶…å‡ºè¨­å®šçš„é™åˆ¶å¾Œï¼Œä»ç„¶å¯èƒ½ç™¼ç”ŸOOMéŒ¯èª¤ã€‚
    args = parse_arguments() # è§£æåƒæ•¸
    seed_every_thing(args["seed"]) # è¨­å®šéš¨æ©Ÿç¨®å­ï¼Œåœ¨æ¯æ¬¡é‹è¡Œæ™‚ç”¢ç”Ÿä¸€è‡´çš„çµæœã€‚
    write_out_dir = path.normpath(path.join(getcwd(), 'reports', args["out_dir"])) # è¼¸å‡ºæ–‡ä»¶çš„å­˜æ”¾è·¯å¾‘
    makedirs(write_out_dir, exist_ok=True)
    
    print('-' * 140)
    print(f'train_mode: {args["train_mode"]} \n')
    
    if args["train_mode"] == 'pre-train': # ä»¥é è¨“ç·´æ¨¡å¼åŸ·è¡Œæ¨¡å‹è¨“ç·´ã€‚
        
        for source in listdir('dataset/source'): # é€å€‹è™•ç†ä¾†æºæ•¸æ“šé›†

            # skip source dataset without pickle file
            data_dir_path = path.join('dataset', 'source', source)
            if not path.exists(f'{data_dir_path}/X_train.pkl'): continue
            
            # make output directory
            write_result_out_dir = path.join(write_out_dir, args["train_mode"], source) # æŒ‡å®šçµæœæª”æ¡ˆ(å«æ¨¡å‹)çš„ä¿å­˜è·¯å¾‘
            makedirs(write_result_out_dir, exist_ok=True)
            
            # load dataset
            X_train, y_train, X_test, y_test = \
                read_data_from_dataset(data_dir_path) # è®€å–'X_train', 'y_train', 'X_test', 'y_test'è³‡æ–™
            X_train = np.concatenate((X_train, X_test), axis=0)  # > no need for test data when pre-training
            y_train = np.concatenate((y_train, y_test), axis=0)  # > no need for test data when pre-training
            X_train, X_valid, y_train, y_valid =  \
                train_test_split(X_train, y_train, test_size=args["valid_ratio"], shuffle=False) # ä¸éš¨æ©Ÿæ‰“äº‚æ•¸æ“š (shuffle=False)
            print(f'\nSource dataset : {source}')
            print(f"ğŸ“Œ X_train.shape: {X_train.shape}")  # æŸ¥çœ‹è¨“ç·´æ•¸æ“šå½¢ç‹€
            print(f"ğŸ“Œ X_valid.shape: {X_valid.shape}")  # æŸ¥çœ‹è¨“ç·´æ•¸æ“šå½¢ç‹€
            print(f'åˆ‡åˆ†æ¯”ä¾‹: {args["valid_ratio"]}')
            
            # construct the model
            sequence_length = 1440  # (period) è¡¨ç¤ºæ™‚é–“æ­¥æ•¸ï¼ˆtime stepsï¼‰ï¼Œå³æ¨¡å‹ä¸€æ¬¡çœ‹å¤šå°‘æ­¥çš„æ­·å²æ•¸æ“šä¾†é€²è¡Œé æ¸¬ã€‚ä¸‹æ¡æ¨£å¾Œå°‡è³‡æ–™é™ç‚ºæˆæ¯åˆ†é˜ä¸€å€‹æ•¸æ“šé»ï¼Œä»¥ 1 å¤© = 1440 åˆ†é˜é€²è¡Œè§€å¯Ÿã€‚
            input_shape = (sequence_length, X_train.shape[1]) # (timesteps, features)ï¼Œperiodè¡¨ç¤ºæ™‚é–“æ­¥æ•¸ï¼ŒX_train.shape[1]ç‚ºæ¬„ä½ç‰¹å¾µã€‚
            input_dim = X_train.shape[1]  # å–å¾—è³‡æ–™é›†çš„ç‰¹å¾µæ•¸
            print(f'sequence_length:{sequence_length}, args["nb_batch"]: {args["nb_batch"]}')
            model, device = build_model(input_shape=(sequence_length, input_dim), gpu=True)

            # TODO: Delete
            # æ¸¬è©¦æ¨¡å‹è¼¸å…¥è¼¸å‡ºçš„æ­£ç¢ºæ€§ (ç¢ºèªæ¨¡å‹æ­£ç¢ºé‹è¡Œ)
            # print("é–‹å§‹æ¸¬è©¦æ¨¡å‹çš„è¼¸å‡º...")
            # x_input = torch.randn(4, sequence_length, input_dim).to(device) # æ¸¬è©¦è¼¸å…¥ batch_size=4, sequence_length, features
            # output = model(x_input)
            # æª¢æŸ¥è¼¸å‡ºå½¢ç‹€æ˜¯å¦æ­£ç¢º
            # expected_shape = (4, 1)  # å‡è¨­è¼¸å‡ºç¶­åº¦æ˜¯ 1
            # assert output.shape == torch.Size(expected_shape), f"æ¨¡å‹è¼¸å‡º shape éŒ¯èª¤: {output.shape}ï¼Œæ‡‰ç‚º {expected_shape}"
            # print(f"æ¸¬è©¦æˆåŠŸï¼æ¨¡å‹è¼¸å‡º shape: {output.shape}\n")  # (batch_size, output_dim)

            # **é€²è¡Œè¨“ç·´**
            # é‡æ–°å¡‘å½¢æ•¸æ“šï¼Œä½¿å…¶ç¬¦åˆ (samples, sequence_length, features)
            # âœ… æŠŠåŸæœ¬çš„è¨“ç·´å’Œé©—è­‰è³‡æ–™è½‰æ›æˆé©åˆ LSTM çš„æ ¼å¼ï¼Œç¢ºä¿ X_train å½¢ç‹€æ­£ç¢ºã€‚
            X_train_seq, y_train_seq = create_sliding_window(X_train, y_train, sequence_length=1440) # å‰µå»ºè¨“ç·´æ•¸æ“š
            X_valid_seq, y_valid_seq = create_sliding_window(X_valid, y_valid, sequence_length=1440) # å‰µå»ºé©—è­‰æ•¸æ“š
            # âœ… è½‰æ›ç‚º PyTorch tensor æ ¼å¼
            # è½‰æˆ tensor æ˜¯ç‚ºäº†è®“æ¨¡å‹èƒ½ä½¿ç”¨ GPU åŠ é€Ÿè¨“ç·´ã€‚
            X_train_tensor = torch.tensor(X_train_seq, dtype=torch.float32) # å‰µå»ºè¨“ç·´æ•¸æ“š
            y_train_tensor = torch.tensor(y_train_seq, dtype=torch.float32)
            X_valid_tensor = torch.tensor(X_valid_seq, dtype=torch.float32) # å‰µå»ºé©—è­‰æ•¸æ“š
            y_valid_tensor = torch.tensor(y_valid_seq, dtype=torch.float32)
            # âœ… å»ºç«‹ PyTorch DataLoader
            # æŠŠè¼¸å…¥å’Œå°æ‡‰çš„æ¨™ç±¤åŒ…æˆä¸€çµ„ï¼Œæ–¹ä¾¿ DataLoader æŠ½æ¨£ã€‚
            train_dataset = TensorDataset(X_train_tensor, y_train_tensor) # å‰µå»ºè¨“ç·´æ•¸æ“š
            val_dataset = TensorDataset(X_valid_tensor, y_valid_tensor) # å‰µå»ºé©—è­‰æ•¸æ“š
            # âœ… åˆ†æ‰¹è®€å–è³‡æ–™
            # shuffle=Falseï¼šä¸æ‰“äº‚é †åºï¼ˆæ™‚é–“åºåˆ—é€šå¸¸è¦ä¿ç•™æ™‚é–“é †åºï¼‰
            # drop_last=Falseï¼šä¿ç•™æœ€å¾Œä¸è¶³ä¸€æ•´æ‰¹çš„è³‡æ–™
            bsize = len(y_train) // args["nb_batch"] # è¨ˆç®—æ‰¹æ¬¡å¤§å°batch_size # --min
            print(f'è¨ˆç®—æ‰¹æ¬¡å¤§å°batch_size: {bsize}')
            train_loader = DataLoader(train_dataset, batch_size=bsize, shuffle=False, drop_last=False) 
            val_loader = DataLoader(val_dataset, batch_size=bsize, shuffle=False, drop_last=False) 
            
            # train the model
            print('é–‹å§‹è¨“ç·´modelæ¨¡å‹ï¼ˆPre-Trainï¼‰')
            Record_args_while_training(write_out_dir, args["train_mode"], source, args['nb_batch'], bsize, sequence_length, data_size=(len(y_train) + len(y_test))) # è¨˜éŒ„åƒæ•¸
            model, train_loss, val_loss, optimizer = train_model(model, train_loader, val_loader, num_epochs=args["nb_epochs"], save_file_path=write_result_out_dir,
                                                                learning_rate=1e-4, device=device, early_stop_patience=10, monitor="val_loss")
            save_lr_curve(train_loss, val_loss, write_result_out_dir, source) # ä¿å­˜æ¯å€‹epochçš„å­¸ç¿’æ›²ç·š

            # prediction (é€²è¡Œé æ¸¬ä¸¦ä¿å­˜çµæœ) ä½¿ç”¨Testingè³‡æ–™è©¦è‘—é æ¸¬ã€‚
            model.eval() # ç¢ºä¿æ¨¡å‹è™•æ–¼è©•ä¼°æ¨¡å¼
            X_test_seq, y_test_seq = create_sliding_window(X_test, y_test, sequence_length=1440)  # å‰µå»ºæ™‚åºçª—å£
            X_test_tensor = torch.tensor(X_test_seq, dtype=torch.float32).to(device) # è½‰æ› X_test ç‚º PyTorch tensor
            # é€²è¡Œæ¨ç†
            with torch.no_grad():
                y_test_pred = model(X_test_tensor)
            y_test_pred = y_test_pred.cpu().numpy() # è½‰æ›ç‚º numpy é™£åˆ—ä»¥ä¾›ç¹ªåœ–
           
            # save log for the model (è¨ˆç®—èª¤å·®æŒ‡æ¨™ä¸¦ä¿å­˜çµæœ)
            y_test = y_test[-len(y_test_pred):] # å°‡y_testçš„é•·åº¦èª¿æ•´ç‚ºèˆ‡ y_test_predï¼ˆæ¨¡å‹é æ¸¬å€¼ï¼‰çš„é•·åº¦ä¸€è‡´ï¼Œç¢ºä¿åœ¨é€²è¡Œè¨ˆç®—å’Œå¯è¦–åŒ–æ™‚ï¼Œå…©è€…é•·åº¦ç›¸ç¬¦ã€‚
            save_prediction_plot(y_test, y_test_pred, write_result_out_dir) # ç¹ªè£½y_testèˆ‡y_test_predçš„å°æ¯”åœ–ï¼Œå±•ç¤ºé æ¸¬å€¼èˆ‡å¯¦éš›å€¼çš„åå·® (æŠ˜ç·šåœ–)
            save_yy_plot(y_test, y_test_pred, write_result_out_dir) # ç¹ªè£½y_testèˆ‡y_test_predçš„å°æ¯”åœ–ï¼Œå±•ç¤ºé æ¸¬å€¼èˆ‡å¯¦éš›å€¼çš„åå·® (æ•£é»åœ–)
            mse_score, rmse_loss, mae_loss, mape_loss, msle_loss, r2 = save_mse(y_test, y_test_pred, write_result_out_dir, model=model, sequence_length=sequence_length, input_dim=input_dim) # è¨ˆç®—y_testå’Œy_test_predä¹‹é–“çš„å‡æ–¹èª¤å·®ï¼ˆMSEï¼‰åˆ†æ•¸ï¼ŒåŒæ™‚å°‡æ¨¡å‹æ‘˜è¦è³‡è¨Šå¯«å…¥æ–‡ä»¶ã€‚
            args["MAE Loss"] = mae_loss
            args["MSE Loss"] = mse_score
            args["RMSE Loss"] = rmse_loss
            args["MAPE Loss"] = mape_loss
            args["MSLE Loss"] = msle_loss
            args["R2 Score"] = r2
            Learning_Rate = optimizer.param_groups[0]["lr"] # å–å¾—åˆå§‹å­¸ç¿’ç‡
            args["Learning Rate"] = Learning_Rate
            save_arguments(args, write_result_out_dir) # ä¿å­˜è¨“ç·´åƒæ•¸ (args) åˆ°çµæœè¼¸å‡ºç›®éŒ„ä¸­ã€‚
            ResidualPlot(y_test, y_test_pred, write_result_out_dir)
            ErrorHistogram(y_test, y_test_pred, write_result_out_dir)

            # clear memory up (æ¸…ç†è¨˜æ†¶é«”ä¸¦ä¿å­˜åƒæ•¸)
            keras.backend.clear_session() # æ¸…ç†è¨˜æ†¶é«”ï¼Œé‡‹æ”¾æ¨¡å‹ä½”ç”¨çš„è³‡æºã€‚
            print('\n' * 2 + '-' * 140 + '\n' * 2)
    

    elif args["train_mode"] == 'transfer-learning': # ä½¿ç”¨é·ç§»å­¸ç¿’ä¾†è¨“ç·´æ¨¡å‹ï¼Œå¾é è¨“ç·´æ¨¡å‹ä¸­æå–æ¬Šé‡ä¸¦æ‡‰ç”¨æ–¼æ–°æ•¸æ“šé›†ã€‚
        
        for target in listdir('dataset/target'):
        
            # skip target in the absence of pickle file
            if not path.exists(f'dataset/target/{target}/X_train.pkl'): continue

            for source in listdir(f'{write_out_dir}/pre-train'): # éæ­·é è¨“ç·´çš„æ¨¡å‹ï¼Œå°æ¯å€‹æ¨¡å‹é€²è¡Œé·ç§»å­¸ç¿’ã€‚
                
                pre_model_path = f'{write_out_dir}/pre-train/{source}/best_model.hdf5' # ç¢ºä¿é è¨“ç·´æ¨¡å‹æ¬Šé‡å­˜åœ¨ã€‚
                if not path.exists(pre_model_path): continue

                # make output directory ä¿å­˜çµæœçš„ç›®éŒ„
                if args["freeze"]:
                    print(f'åœ¨é·ç§»å­¸ç¿’ä¸­ï¼Œæ˜¯å¦å‡çµæ¬Šé‡: {args["freeze"]}ï¼Œå³å‡çµæ¬Šé‡ã€‚')
                    train_mode = f'{args["train_mode"]} (Freeze)'
                else:
                    print(f'åœ¨é·ç§»å­¸ç¿’ä¸­ï¼Œæ˜¯å¦å‡çµæ¬Šé‡: {args["freeze"]}ï¼Œå³è§£å‡æ¬Šé‡ã€‚')
                    train_mode = f'{args["train_mode"]} (Unfreeze)'
                write_result_out_dir = path.join(write_out_dir, train_mode, target, source)
                makedirs(write_result_out_dir, exist_ok=True)
                    
                # load dataset (åŠ è¼‰ç›®æ¨™æ•¸æ“šé›†)
                data_dir_path = f'dataset/target/{target}'
                X_train, y_train, X_test, y_test = \
                    read_data_from_dataset(data_dir_path)
                period = 1440 # periodï¼šè¡¨ç¤ºæ™‚é–“æ­¥æ•¸ï¼ˆtime stepsï¼‰ï¼Œå³æ¨¡å‹ä¸€æ¬¡çœ‹å¤šå°‘æ­¥çš„æ­·å²æ•¸æ“šä¾†é€²è¡Œé æ¸¬ã€‚ä¸‹æ¡æ¨£å¾Œå°‡è³‡æ–™é™ç‚ºæˆæ¯åˆ†é˜ä¸€å€‹æ•¸æ“šé»ï¼Œä»¥ 1 å¤© = 1440 åˆ†é˜é€²è¡Œè§€å¯Ÿã€‚ 
                X_train, X_valid, y_train, y_valid = \
                    train_test_split(X_train, y_train, test_size=args["valid_ratio"], shuffle=False) # å°‡è¨“ç·´é›†åˆ†å‰²ç‚ºè¨“ç·´å’Œé©—è­‰ã€‚
                print(f'\nTarget dataset : {target}')
                print(f'\nSource dataset : {source}')
                print(f'\nX_train : {X_train.shape[0]}')
                print(f'\nX_valid : {X_valid.shape[0]}')
                print(f'\nX_test : {X_test.shape[0]}')
                print(f'period:{period}, args["nb_batch"]: {args["nb_batch"]}')
                
                # construct the model (æ§‹å»ºä¸¦ç·¨è­¯æ¨¡å‹)
                pre_model = load_model(pre_model_path, custom_objects={'rmse': rmse}) # åŠ è¼‰é è¨“ç·´æ¨¡å‹çš„æ¬Šé‡ã€‚
                file_path = path.join(write_result_out_dir, f'{source}_transferred_best_model.hdf5')
                callbacks = make_callbacks(file_path)
                input_shape = (period, X_train.shape[1]) # (timesteps, features)ï¼Œperiodè¡¨ç¤ºæ™‚é–“æ­¥æ•¸ï¼ŒX_train.shape[1]ç‚ºæ¬„ä½ç‰¹å¾µã€‚
                print('é–‹å§‹å»ºç«‹æ¨¡å‹ï¼ˆTransfer-Learningï¼‰')
                model = build_model(input_shape, args["gpu"], write_result_out_dir, pre_model=pre_model, freeze=args["freeze"]) # æ§‹å»ºé·ç§»å­¸ç¿’æ¨¡å‹ # freezeåƒæ•¸æ±ºå®šæ˜¯å¦å‡çµé è¨“ç·´æ¨¡å‹çš„å±¤ï¼Œä»¥é¿å…åœ¨é·ç§»å­¸ç¿’ä¸­å¾®èª¿å®ƒå€‘ã€‚
        
                # train the model (è¨“ç·´æ¨¡å‹)
                bsize = len(y_train) // args["nb_batch"] # è¨ˆç®—æ‰¹æ¬¡å¤§å°batch_size # --min
                print(f'è¨ˆç®—æ‰¹æ¬¡å¤§å°batch_size: {bsize}')
                RTG = ReccurentTrainingGenerator(X_train, y_train, batch_size=bsize, timesteps=period, delay=1) # ç”Ÿæˆè¨“ç·´æ•¸æ“šï¼Œä»¥æ‰¹æ¬¡å½¢å¼æä¾›çµ¦æ¨¡å‹ã€‚
                RVG = ReccurentTrainingGenerator(X_valid, y_valid, batch_size=bsize, timesteps=period, delay=1) # ç”Ÿæˆé©—è­‰æ•¸æ“šï¼Œä»¥æ‰¹æ¬¡å½¢å¼æä¾›çµ¦æ¨¡å‹ã€‚
                Record_args_while_training(write_out_dir, train_mode, target, args['nb_batch'], bsize, period, data_size=(len(y_train) + len(y_valid) + len(y_test)))
                H = model.fit_generator(RTG, validation_data=RVG, epochs=args["nb_epochs"], verbose=1, callbacks=callbacks) # è¨“ç·´æ¨¡å‹
                save_lr_curve(H, write_result_out_dir, target) # ç¹ªè£½å­¸ç¿’æ›²ç·š
                
                # prediction (é€²è¡Œé æ¸¬ä¸¦ä¿å­˜çµæœ)
                best_model = load_model(file_path, custom_objects={'rmse': rmse}) # åŠ è¼‰æ¨¡å‹
                RPG = ReccurentPredictingGenerator(X_test, batch_size=1, timesteps=period) # ç”Ÿæˆæ¸¬è©¦æ•¸æ“šã€‚
                y_test_pred = best_model.predict_generator(RPG) # é æ¸¬æ¸¬è©¦æ•¸æ“š

                # save log for the model (è¨ˆç®—MSEä¸¦ä¿å­˜çµæœ)
                y_test = y_test[-len(y_test_pred):] # å°‡y_testçš„é•·åº¦èª¿æ•´ç‚ºèˆ‡ y_test_predï¼ˆæ¨¡å‹é æ¸¬å€¼ï¼‰çš„é•·åº¦ä¸€è‡´ï¼Œç¢ºä¿åœ¨é€²è¡Œè¨ˆç®—å’Œå¯è¦–åŒ–æ™‚ï¼Œå…©è€…é•·åº¦ç›¸ç¬¦ã€‚ã€‚
                save_prediction_plot(y_test, y_test_pred, write_result_out_dir) # ç¹ªè£½y_testèˆ‡y_test_predçš„å°æ¯”åœ–ï¼Œå±•ç¤ºé æ¸¬å€¼èˆ‡å¯¦éš›å€¼çš„åå·® (æŠ˜ç·šåœ–)
                save_yy_plot(y_test, y_test_pred, write_result_out_dir) # ç¹ªè£½y_testèˆ‡y_test_predçš„å°æ¯”åœ–ï¼Œå±•ç¤ºé æ¸¬å€¼èˆ‡å¯¦éš›å€¼çš„åå·® (æ•£é»åœ–)
                mse_score, rmse_loss, mae_loss, mape_loss, msle_loss, r2 = save_mse(y_test, y_test_pred, write_result_out_dir, model=best_model) # è¨ˆç®—y_testå’Œy_test_predä¹‹é–“çš„å‡æ–¹èª¤å·®ï¼ˆMSEï¼‰åˆ†æ•¸ï¼ŒåŒæ™‚å°‡æ¨¡å‹æ‘˜è¦è³‡è¨Šå¯«å…¥æ–‡ä»¶ã€‚
                args["MAE Loss"] = mae_loss
                args["MSE Loss"] = mse_score
                args["RMSE Loss"] = rmse_loss
                args["MAPE Loss"] = mape_loss
                args["MSLE Loss"] = msle_loss
                args["R2 Score"] = r2
                save_arguments(args, write_result_out_dir) # ä¿å­˜æœ¬æ¬¡è¨“ç·´æˆ–æ¸¬è©¦çš„æ‰€æœ‰åƒæ•¸è¨­å®šåŠçµæœã€‚
                ResidualPlot(y_test, y_test_pred, write_result_out_dir)
                ErrorHistogram(y_test, y_test_pred, write_result_out_dir)

                # clear memory up (æ¸…ç†è¨˜æ†¶é«”ä¸¦ä¿å­˜åƒæ•¸)
                keras.backend.clear_session() # é‡‹æ”¾è¨˜æ†¶é«”
                print('\n' * 2 + '-' * 140 + '\n' * 2)
    

    elif args["train_mode"] == 'without-transfer-learning': # ä¸ä½¿ç”¨é·ç§»å­¸ç¿’

        for target in listdir('dataset/target'):
        
            # make output directory
            write_result_out_dir = path.join(write_out_dir, args["train_mode"], target)
            makedirs(write_result_out_dir, exist_ok=True)

            # load dataset (åŠ è¼‰æ•¸æ“šé›†ä¸¦åˆ†å‰²ç‚ºè¨“ç·´å’Œé©—è­‰é›†)
            data_dir_path = path.join('dataset', 'target', target)
            X_train, y_train, X_test, y_test = \
                read_data_from_dataset(data_dir_path) # è®€å–'X_train', 'y_train', 'X_test', 'y_test'è³‡æ–™
            period = 1440 # periodï¼šè¡¨ç¤ºæ™‚é–“æ­¥æ•¸ï¼ˆtime stepsï¼‰ï¼Œå³æ¨¡å‹ä¸€æ¬¡çœ‹å¤šå°‘æ­¥çš„æ­·å²æ•¸æ“šä¾†é€²è¡Œé æ¸¬ã€‚ä¸‹æ¡æ¨£å¾Œå°‡è³‡æ–™é™ç‚ºæˆæ¯åˆ†é˜ä¸€å€‹æ•¸æ“šé»ï¼Œä»¥ 1 å¤© = 1440 åˆ†é˜é€²è¡Œè§€å¯Ÿã€‚
            X_train, X_valid, y_train, y_valid =  \
                train_test_split(X_train, y_train, test_size=args["valid_ratio"], shuffle=False) # ä¸éš¨æ©Ÿæ‰“äº‚æ•¸æ“š (shuffle=False)
            print(f'\nTarget dataset : {target}')
            print(f'\nX_train : {X_train.shape[0]}')
            print(f'\nX_valid : {X_valid.shape[0]}')
            print(f'\nX_test : {X_test.shape[0]}')
            print(f'period:{period}, args["nb_batch"]: {args["nb_batch"]}')
            
            # construct the model (æ§‹å»ºæ¨¡å‹)
            file_path = path.join(write_result_out_dir, 'best_model.hdf5')
            callbacks = make_callbacks(file_path)
            input_shape = (period, X_train.shape[1])
            model = build_model(input_shape, args["gpu"], write_result_out_dir)
            
            # train the model (è¨“ç·´æ¨¡å‹)
            bsize = len(y_train) // args["nb_batch"] # è¨ˆç®—æ‰¹æ¬¡å¤§å°batch_size # --min
            print(f'è¨ˆç®—æ‰¹æ¬¡å¤§å°batch_size: {bsize}')
            RTG = ReccurentTrainingGenerator(X_train, y_train, batch_size=bsize, timesteps=period, delay=1) # ç”Ÿæˆè¨“ç·´æ•¸æ“šï¼Œä»¥æ‰¹æ¬¡å½¢å¼æä¾›çµ¦æ¨¡å‹ã€‚
            RVG = ReccurentTrainingGenerator(X_valid, y_valid, batch_size=bsize, timesteps=period, delay=1) # ç”Ÿæˆé©—è­‰æ•¸æ“šï¼Œä»¥æ‰¹æ¬¡å½¢å¼æä¾›çµ¦æ¨¡å‹ã€‚
            print('é–‹å§‹è¨“ç·´modelæ¨¡å‹ï¼ˆWithout-Transfer-Learningï¼‰')
            Record_args_while_training(write_out_dir, args["train_mode"], target, args['nb_batch'], bsize, period, data_size=(len(y_train) + len(y_valid) + len(y_test)))
            H = model.fit_generator(RTG, validation_data=RVG, epochs=args["nb_epochs"], verbose=1, callbacks=callbacks) # è¨“ç·´æ¨¡å‹
            save_lr_curve(H, write_result_out_dir, target) # ç¹ªè£½å­¸ç¿’æ›²ç·š

            # prediction (é æ¸¬)
            best_model = load_model(file_path, custom_objects={'rmse': rmse}) # å‚³érmseè‡ªå®šç¾©æŒ‡æ¨™
            RPG = ReccurentPredictingGenerator(X_test, batch_size=1, timesteps=period) # ç”Ÿæˆæ¸¬è©¦æ•¸æ“šã€‚
            y_test_pred = best_model.predict_generator(RPG) # é æ¸¬æ¸¬è©¦æ•¸æ“š

            # save log for the model (è¨ˆç®—MSEèª¤å·®å’Œä¿å­˜çµæœ)
            y_test = y_test[-len(y_test_pred):] # å°‡y_testçš„é•·åº¦èª¿æ•´ç‚ºèˆ‡ y_test_predï¼ˆæ¨¡å‹é æ¸¬å€¼ï¼‰çš„é•·åº¦ä¸€è‡´ï¼Œç¢ºä¿åœ¨é€²è¡Œè¨ˆç®—å’Œå¯è¦–åŒ–æ™‚ï¼Œå…©è€…é•·åº¦ç›¸ç¬¦ã€‚
            save_prediction_plot(y_test, y_test_pred, write_result_out_dir) # ç¹ªè£½y_testèˆ‡y_test_predçš„å°æ¯”åœ–ï¼Œå±•ç¤ºé æ¸¬å€¼èˆ‡å¯¦éš›å€¼çš„åå·® (æŠ˜ç·šåœ–)
            save_yy_plot(y_test, y_test_pred, write_result_out_dir) # ç¹ªè£½y_testèˆ‡y_test_predçš„å°æ¯”åœ–ï¼Œå±•ç¤ºé æ¸¬å€¼èˆ‡å¯¦éš›å€¼çš„åå·® (æ•£é»åœ–)
            mse_score, rmse_loss, mae_loss, mape_loss, msle_loss, r2 = save_mse(y_test, y_test_pred, write_result_out_dir, model=best_model) # è¨ˆç®—y_testå’Œy_test_predä¹‹é–“çš„å‡æ–¹èª¤å·®ï¼ˆMSEï¼‰åˆ†æ•¸ï¼Œ
            args["MAE Loss"] = mae_loss
            args["MSE Loss"] = mse_score
            args["RMSE Loss"] = rmse_loss
            args["MAPE Loss"] = mape_loss
            args["MSLE Loss"] = msle_loss
            args["R2 Score"] = r2
            save_arguments(args, write_result_out_dir) # ä¿å­˜æœ¬æ¬¡è¨“ç·´æˆ–æ¸¬è©¦çš„æ‰€æœ‰åƒæ•¸è¨­å®šåŠçµæœã€‚
            ResidualPlot(y_test, y_test_pred, write_result_out_dir)
            ErrorHistogram(y_test, y_test_pred, write_result_out_dir)

            # clear memory up (æ¸…ç†è¨˜æ†¶é«”)
            keras.backend.clear_session()
            print('\n' * 2 + '-' * 140 + '\n' * 2)
    
    
    elif args["train_mode"] == 'comparison': # æ¯”è¼ƒ Transfer-Learningé·ç§»å­¸ç¿’ vs. Without-Transfer-Learningä¸ä½¿ç”¨é·ç§»å­¸ç¿’
        out_dir, train_mode = write_out_dir, args["train_mode"]
        metrics_comparison(out_dir, train_mode) # æ¯”è¼ƒæ‰€æœ‰metricsã€‚
        MSE_Improvement(out_dir, train_mode) # æ¯”è¼ƒMSE
        MAE_Improvement(out_dir, train_mode) # æ¯”è¼ƒMAE
        dataset_idx_vs_improvement(out_dir, train_mode) # 'DTW'
    
    
    elif args["train_mode"] == 'ensemble': # ä½¿ç”¨ensembleæ•´é«”å­¸ç¿’ã€‚é€šéèšåˆã€ä»¥å¹³å‡çš„æ–¹å¼ä¾†å¾—åˆ°æœ€çµ‚é æ¸¬çµæœ
        
        for target in listdir('dataset/target'):    
            
            # make output directory
            TL_model_dir = path.join(write_out_dir, args["train_mode"], target, 'model')
            makedirs(TL_model_dir, exist_ok=True) # å»ºç«‹ç›®æ¨™ç›®éŒ„ï¼ˆå¦‚æœä¸å­˜åœ¨ï¼‰ 
            
            # å°‡transfer-learning (Unfreeze)çš„æ¨¡å‹è¤‡è£½æ¬ç§»åˆ°ensembleåº•ä¸‹çš„modelè³‡æ–™å¤¾
            source_dir = path.join(write_out_dir,'transfer-learning (Unfreeze)', target) # ç²å–æ¨¡å‹ä¾†æºè³‡æ–™å¤¾
            for TL_model in listdir(source_dir):
                src_path = path.join(source_dir, TL_model, f'{TL_model}_transferred_best_model.hdf5')
                if path.isfile(src_path): # æª¢æŸ¥æ˜¯å¦æ˜¯æª”æ¡ˆå†åŸ·è¡Œè¤‡è£½
                    shutil.copy(src_path, TL_model_dir) # è¤‡è£½æª”æ¡ˆåˆ°ç›®æ¨™è³‡æ–™å¤¾ï¼ˆè¦†è“‹æ—¢æœ‰æª”æ¡ˆï¼‰
                    print(f"å·²è¤‡è£½: {src_path} -> {TL_model_dir}")
            print("æ¨¡å‹è¤‡è£½å®Œæˆã€‚")
                    
            # ensembleæ•´é«”å­¸ç¿’ é æ¸¬èˆ‡è©•ä¼°ã€‚
            period = 1440 # periodï¼šè¡¨ç¤ºæ™‚é–“æ­¥æ•¸ï¼ˆtime stepsï¼‰ï¼Œå³æ¨¡å‹ä¸€æ¬¡çœ‹å¤šå°‘æ­¥çš„æ­·å²æ•¸æ“šä¾†é€²è¡Œé æ¸¬ã€‚ä¸‹æ¡æ¨£å¾Œå°‡è³‡æ–™é™ç‚ºæˆæ¯åˆ†é˜ä¸€å€‹æ•¸æ“šé»ï¼Œä»¥ 1 å¤© = 1440 åˆ†é˜é€²è¡Œè§€å¯Ÿã€‚
            start_ensemble (period, write_out_dir=path.join(write_out_dir, args["train_mode"]))
            keras.backend.clear_session() # æ¸…ç†è¨˜æ†¶é«”
            print('\n' * 2 + '-' * 140 + '\n' * 2)        
        

    # elif args["train_mode"] == 'bagging': # ä½¿ç”¨Baggingé›†æˆå¼å­¸ç¿’ã€‚é€šéå°æ•¸æ“šé›†é€²è¡Œå¤šæ¬¡é‡æŠ½æ¨£ï¼Œç”Ÿæˆå¤šå€‹è¨“ç·´å­é›†ï¼Œä¸¦åœ¨é€™äº›å­é›†ä¸Šè¨“ç·´å¤šå€‹æ¨¡å‹ï¼Œæœ€çµ‚é€šéèšåˆä¾†æå‡é æ¸¬ç©©å®šæ€§ã€‚å¦‚éš¨æ©Ÿæ£®æ—ç®—æ³•ã€‚
    
    #     for target in listdir('dataset/target'):
            
    #         # make output directory
    #         write_result_out_dir = path.join(write_out_dir, args["train_mode"], target)
    #         makedirs(write_result_out_dir, exist_ok=True)

    #         # load dataset (åŠ è¼‰æ•¸æ“šé›†)
    #         data_dir_path = path.join('dataset', 'target', target)
    #         X_train, y_train, X_test, y_test = \
    #             read_data_from_dataset(data_dir_path)
    #         period = 1440 # periodï¼šè¡¨ç¤ºæ™‚é–“æ­¥æ•¸ï¼ˆtime stepsï¼‰ï¼Œå³æ¨¡å‹ä¸€æ¬¡çœ‹å¤šå°‘æ­¥çš„æ­·å²æ•¸æ“šä¾†é€²è¡Œé æ¸¬ã€‚ä¸‹æ¡æ¨£å¾Œå°‡è³‡æ–™é™ç‚ºæˆæ¯åˆ†é˜ä¸€å€‹æ•¸æ“šé»ï¼Œä»¥ 1 å¤© = 1440 åˆ†é˜é€²è¡Œè§€å¯Ÿã€‚

            # # make subsets (è¨ˆç®—æœ€ä½³å€å¡Šé•·åº¦ä¸¦ç”Ÿæˆè¨“ç·´å­é›†)
            # b_star = optimal_block_length(y_train) # è¨ˆç®—æœ€ä½³çš„å€å¡Šé•·åº¦ï¼ˆb_starï¼‰ï¼Œç„¶å¾Œä½¿ç”¨è©²é•·åº¦ä¾†ç”Ÿæˆé©åˆæ™‚é–“ä¾è³´æ€§çš„æ•¸æ“šå­é›†ã€‚
            # b_star_cb = math.ceil(b_star[0].b_star_cb) # å‘ä¸Šå–æ•´ï¼Œç¢ºä¿å€å¡Šé•·åº¦ç‚ºæ•´æ•¸ã€‚
            # print(f'optimal block length for circular bootstrap = {b_star_cb}')
            # subsets_y_train = circular_block_bootstrap(y_train, block_length=b_star_cb,
            #                                            replications=args["nb_subset"], replace=True) # æ ¹æ“šè¨ˆç®—å‡ºçš„å€å¡Šé•·åº¦ï¼Œå° y_train é€²è¡Œ nb_subset æ¬¡é‡æŠ½æ¨£ï¼Œç”Ÿæˆå¤šå€‹å­é›†ã€‚
            # subsets_X_train = []
            # for i in range(X_train.shape[1]): # å°X_trainçš„æ¯å€‹ç‰¹å¾µä½¿ç”¨ç›¸åŒçš„æ–¹æ³•é€²è¡Œ Circular Block Bootstrap é‡æŠ½æ¨£ï¼Œç”Ÿæˆå¤šå€‹ X_train å­é›†ï¼Œä¸¦é‡æ–°æ’åˆ—ä»¥åŒ¹é…æ¨¡å‹è¼¸å…¥æ ¼å¼ã€‚
            #     np.random.seed(0) # ç¢ºä¿é‡ç¾æ€§
            #     X_cb = circular_block_bootstrap(X_train[:, i], block_length=b_star_cb,
            #                                     replications=args["nb_subset"], replace=True) # å°æ¯å€‹ç‰¹å¾µè³‡æ–™é€²è¡Œé‡æŠ½æ¨£ï¼Œç”Ÿæˆå¤šå€‹å­é›†ï¼Œä¸¦å°‡çµæœå„²å­˜åˆ° subsets_X_train åˆ—è¡¨ä¸­ã€‚
            #     subsets_X_train.append(X_cb)
            # # 1.) å°æ¯å€‹ç‰¹å¾µé€²è¡Œé‡æŠ½æ¨£ï¼Œå› æ­¤len(subsets_X_train)é•·åº¦ç‚ºç‰¹å¾µæ•¸ï¼Œè€Œæ¯å€‹å…ƒç´ çš„å½¢ç‹€ç‚º(nb_subset, n_samples)ã€‚å³ subsets_X_train åˆ—è¡¨çš„çµæ§‹æ˜¯ [n_featureså€‹å…ƒç´ ï¼Œæ¯å€‹å…ƒç´ çš„å½¢ç‹€æ˜¯(nb_subset, n_samples)]ã€‚
            # subsets_X_train = np.array(subsets_X_train) # 2.) è½‰æ›ç‚ºNumPyé™£åˆ—ï¼Œè®Šæˆ3Dé™£åˆ—ï¼Œå½¢ç‹€ç‚º(n_features, nb_subset, n_samples)ã€‚
            # subsets_X_train = subsets_X_train.transpose(1, 2, 0) # 3.) ä½¿ç”¨transposeè½‰ç½®æ–¹æ³•èª¿æ•´å…¶å½¢ç‹€ï¼Œä½¿å…¶ç¬¦åˆæ¨¡å‹è¼¸å…¥çš„æ ¼å¼ã€‚å½¢ç‹€è®Šç‚º (nb_subsetå­é›†æ•¸é‡, n_samplesæ¨£æœ¬æ•¸, n_featuresç‰¹å¾µæ•¸)

            # # train the model for each subset (å°æ¯å€‹å­é›†è¨“ç·´æ¨¡å‹)
            # model_dir = path.join(write_result_out_dir, 'model')
            # makedirs(model_dir, exist_ok=True)
            # for i_subset, (i_X_train, i_y_train) in enumerate(zip(subsets_X_train, subsets_y_train)): # ç•¶å°subsets_X_trainé€²è¡Œè¿­ä»£æ™‚ï¼Œæ¯æ¬¡å–å‡ºçš„i_X_trainçš„å½¢ç‹€æ˜¯(n_samples, n_features)
                
            #     print(f'i_X_train.shape, i_y_train.shape: {i_X_train.shape, i_y_train.shape}')
                
            #     i_X_train, i_X_valid, i_y_train, i_y_valid = \
            #         train_test_split(i_X_train, i_y_train, test_size=args["valid_ratio"], shuffle=False) # æ¯å€‹å­é›†åˆ†æˆè¨“ç·´é›†å’Œé©—è­‰é›†ã€‚
                
            #     # construct the model (æ¯å€‹å­é›†å°‡æœƒè¨“ç·´ä¸€å€‹æ¨¡å‹ï¼Œé€™äº›æ¨¡å‹æœ€çµ‚å°‡è¢«é›†åˆä½¿ç”¨ï¼Œä»¥å¢åŠ é æ¸¬çš„ç©©å®šæ€§å’Œæ³›åŒ–èƒ½åŠ›ã€‚)
            #     file_path = path.join(model_dir, f'{target}_best_model_{i_subset}.hdf5')
            #     callbacks = make_callbacks(file_path, save_csv=False)
            #     input_shape = (period, i_X_train.shape[1])  # subsets_X_train.shape[2] is number of variableï¼Œå› æ­¤i_X_train.shape[1] å°æ‡‰çš„æ˜¯ç‰¹å¾µæ•¸ï¼Œå³ n_featuresã€‚
            #     print(f'input_shape: {input_shape}')
            #     model = build_model(input_shape, args["gpu"], write_result_out_dir, savefig=False)

            #     # train the model
            #     bsize = len(i_y_train) // args["nb_batch"]
            #     RTG = ReccurentTrainingGenerator(i_X_train, i_y_train, batch_size=bsize, timesteps=period, delay=1) # ç”Ÿæˆè¨“ç·´æ•¸æ“šï¼Œä»¥æ‰¹æ¬¡å½¢å¼æä¾›çµ¦æ¨¡å‹ã€‚
            #     RVG = ReccurentTrainingGenerator(i_X_valid, i_y_valid, batch_size=bsize, timesteps=period, delay=1) # ç”Ÿæˆé©—è­‰æ•¸æ“šï¼Œä»¥æ‰¹æ¬¡å½¢å¼æä¾›çµ¦æ¨¡å‹ã€‚
            #     Record_args_while_training(write_out_dir, args["train_mode"], target, args['nb_batch'], bsize, period, data_size=(len(y_train) + len(y_test)))
            #     H = model.fit_generator(RTG, validation_data=RVG, epochs=args["nb_epochs"], verbose=1, callbacks=callbacks) # è¨“ç·´æ¨¡å‹
            
            # keras.backend.clear_session() # æ¸…ç†è¨˜æ†¶é«”
            # print('\n' * 2 + '-' * 140 + '\n' * 2)

    
    elif args["train_mode"] == 'noise-injection': # æ·»åŠ éš¨æ©Ÿå™ªè²ä¾†è¨“ç·´æ¨¡å‹ï¼Œä½¿æ¨¡å‹åœ¨è¨“ç·´éç¨‹ä¸­é‡åˆ°æ›´å¤šçš„æ•¸æ“šè®ŠåŒ–ï¼Œæ¸›å°‘éæ“¬åˆä¸¦æé«˜æ¨¡å‹å°æ¸¬è©¦æ•¸æ“šçš„æ³›åŒ–èƒ½åŠ›ã€‚

        for target in listdir('dataset/target'):
            
            # make output directory (è¨­ç½®è¼¸å‡ºç›®éŒ„)
            write_result_out_dir = path.join(write_out_dir, args["train_mode"], target)
            makedirs(write_result_out_dir, exist_ok=True)

            # load dataset (åŠ è¼‰æ•¸æ“šé›†ä¸¦åˆ‡åˆ†ç‚ºè¨“ç·´å’Œé©—è­‰é›†)
            data_dir_path = path.join('dataset', 'target', target)
            X_train, y_train, X_test, y_test = \
                read_data_from_dataset(data_dir_path)
            period = 1440 # periodï¼šè¡¨ç¤ºæ™‚é–“æ­¥æ•¸ï¼ˆtime stepsï¼‰ï¼Œå³æ¨¡å‹ä¸€æ¬¡çœ‹å¤šå°‘æ­¥çš„æ­·å²æ•¸æ“šä¾†é€²è¡Œé æ¸¬ã€‚ä¸‹æ¡æ¨£å¾Œå°‡è³‡æ–™é™ç‚ºæˆæ¯åˆ†é˜ä¸€å€‹æ•¸æ“šé»ï¼Œä»¥ 1 å¤© = 1440 åˆ†é˜é€²è¡Œè§€å¯Ÿã€‚
            X_train, X_valid, y_train, y_valid =  \
                train_test_split(X_train, y_train, test_size=args["valid_ratio"], shuffle=False) # å°‡è¨“ç·´æ•¸æ“šåŠƒåˆ†ç‚ºè¨“ç·´é›†å’Œé©—è­‰é›†ã€‚
            print(f'\nTarget dataset : {target}')
            print(f'\nX_train : {X_train.shape}')
            print(f'\nX_valid : {X_valid.shape}')
            print(f'\nX_test : {X_test.shape[0]}')

            # construct the model
            file_path = path.join(write_result_out_dir, 'best_model.hdf5')
            callbacks = make_callbacks(file_path)
            input_shape = (period, X_train.shape[1])
            model = build_model(input_shape, args["gpu"], write_result_out_dir, noise=args["noise_var"])

            # train the model
            bsize = len(y_train) // args["nb_batch"]
            RTG = ReccurentTrainingGenerator(X_train, y_train, batch_size=bsize, timesteps=period, delay=1) # ç”Ÿæˆè¨“ç·´æ•¸æ“šï¼Œä»¥æ‰¹æ¬¡å½¢å¼æä¾›çµ¦æ¨¡å‹ã€‚
            RVG = ReccurentTrainingGenerator(X_valid, y_valid, batch_size=bsize, timesteps=period, delay=1) # ç”Ÿæˆé©—è­‰æ•¸æ“šï¼Œä»¥æ‰¹æ¬¡å½¢å¼æä¾›çµ¦æ¨¡å‹ã€‚
            Record_args_while_training(write_out_dir, args["train_mode"], target, args['nb_batch'], bsize, period, data_size=(len(y_train) + len(y_valid) + len(y_test)))
            H = model.fit_generator(RTG, validation_data=RVG, epochs=args["nb_epochs"], verbose=1, callbacks=callbacks) # è¨“ç·´æ¨¡å‹
            save_lr_curve(H, write_result_out_dir, target) # ç¹ªè£½å­¸ç¿’æ›²ç·š

            # prediction
            best_model = load_model(file_path)
            RPG = ReccurentPredictingGenerator(X_test, batch_size=1, timesteps=period) # ç”Ÿæˆæ¸¬è©¦æ•¸æ“šã€‚
            y_test_pred = best_model.predict_generator(RPG) # é æ¸¬æ¸¬è©¦æ•¸æ“š

            # save log for the model
            y_test = y_test[-len(y_test_pred):] # å°‡y_testçš„é•·åº¦èª¿æ•´ç‚ºèˆ‡ y_test_predï¼ˆæ¨¡å‹é æ¸¬å€¼ï¼‰çš„é•·åº¦ä¸€è‡´ï¼Œç¢ºä¿åœ¨é€²è¡Œè¨ˆç®—å’Œå¯è¦–åŒ–æ™‚ï¼Œå…©è€…é•·åº¦ç›¸ç¬¦ã€‚
            save_prediction_plot(y_test, y_test_pred, write_result_out_dir) # ç¹ªè£½y_testèˆ‡y_test_predçš„å°æ¯”åœ–ï¼Œå±•ç¤ºé æ¸¬å€¼èˆ‡å¯¦éš›å€¼çš„åå·® (æŠ˜ç·šåœ–)
            save_yy_plot(y_test, y_test_pred, write_result_out_dir) # ç¹ªè£½y_testèˆ‡y_test_predçš„å°æ¯”åœ–ï¼Œå±•ç¤ºé æ¸¬å€¼èˆ‡å¯¦éš›å€¼çš„åå·® (æ•£é»åœ–)
            mse_score = save_mse(y_test, y_test_pred, write_result_out_dir, model=best_model) # è¨ˆç®—y_testå’Œy_test_predä¹‹é–“çš„å‡æ–¹èª¤å·®ï¼ˆMSEï¼‰åˆ†æ•¸ï¼Œ
            args["mse"] = mse_score
            save_arguments(args, write_result_out_dir) # ä¿å­˜æœ¬æ¬¡è¨“ç·´æˆ–æ¸¬è©¦çš„æ‰€æœ‰åƒæ•¸è¨­å®šåŠçµæœã€‚

            # clear memory up (æ¸…ç†è¨˜æ†¶é«”)
            keras.backend.clear_session()
            print('\n' * 2 + '-' * 140 + '\n' * 2)

    else:
        print('No matchining train_mode')

if __name__ == '__main__':
    main()
