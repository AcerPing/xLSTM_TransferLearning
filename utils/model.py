import os
import numpy as np
import pandas as pd

# TODO: Delete
# import tensorflow as tf
# from keras.layers import Input, Dense, BatchNormalization
# from keras.layers import TimeDistributed # wrappers
# from keras.layers import GaussianNoise # noise
# from keras.models import Model
# from keras.utils import plot_model
# from keras.optimizers import Adam
# from keras import initializers, regularizers
# import keras.backend as K

# pytorch
import torch
import torch.nn as nn
import torch.optim as optim
from torchinfo import summary

# å–å¾—åŸå§‹ include_paths()ï¼Œé¿å…éè¿´å‘¼å«è‡ªå·±
_original_include_paths = torch.utils.cpp_extension.include_paths
def include_paths_patched(*args, **kwargs):
    if 'cuda' in kwargs:
        del kwargs['cuda']  # ç§»é™¤ä¸æ”¯æ´çš„åƒæ•¸
    return _original_include_paths(*args, **kwargs)
torch.utils.cpp_extension.include_paths = include_paths_patched # é€²è¡Œ Monkey Patch
print("âœ… Patched torch.utils.cpp_extension.include_paths successfully!")

import xlstm
from xlstm import xLSTMBlockStack, xLSTMBlockStackConfig  # åŒ¯å…¥xLSTMæ‰€éœ€çš„é¡åˆ¥
from xlstm.blocks.slstm.block import sLSTMBlockConfig  # sLSTMè¨­å®š
from xlstm.blocks.slstm.block import sLSTMBlock


class LinearHeadwiseExpandConfig:
    '''
    å°è£æ¨¡å‹çš„ã€Œè¼¸å…¥ç‰¹å¾µæ•¸é‡ã€èˆ‡ã€Œé ­æ•¸ï¼ˆnum_headsï¼‰ã€çš„çµ„åˆè¨­å®šï¼Œ
    æŠŠä¸€å€‹è¼¸å…¥å‘é‡åˆ†æˆå¤šå€‹headè™•ç†ã€‚
    "æŠŠåŸæœ¬çš„ç‰¹å¾µå‘é‡åˆ‡æˆ N ä»½ï¼Œåˆ†åˆ¥ç”¨ä¸åŒçš„ Linear è™•ç†å¾Œå†æ‹¼èµ·ä¾†"
    "è¼¸å…¥å‘é‡ â†’ åˆ‡æˆ N ä»½ â†’ å„è·‘è‡ªå·± Linear â†’ æ‹¼èµ·ä¾†"
    '''
    def __init__(self, in_features, num_heads):
        self.in_features = in_features # è¼¸å…¥çš„ç‰¹å¾µæ•¸ï¼ˆé€šå¸¸æ˜¯embeddingç¶­åº¦ï¼‰ã€‚
        self.num_heads = num_heads # è¦åˆ†æˆå¹¾å€‹ã€Œé ­ã€ä¾†ä¸¦è¡Œé‹ç®—ã€‚
        # print(f"ğŸ” Debug: LinearHeadwiseExpandConfig - in_features={self.in_features}, num_heads={self.num_heads}") # è¼¸å‡ºç›®å‰çš„è¨­å®šå€¼
        # æŠŠè¼¸å…¥ç‰¹å¾µåˆ‡æˆ num_heads å€‹éƒ¨åˆ†ï¼Œæ¯å€‹ head è¦è™•ç†ç›¸åŒé•·åº¦çš„å‘é‡ã€‚
        assert self.in_features % self.num_heads == 0, \
            f"âš ï¸ AssertionError: in_features ({self.in_features}) å¿…é ˆæ˜¯ num_heads ({self.num_heads}) çš„å€æ•¸" # æª¢æŸ¥ in_features å¿…é ˆå¯ä»¥æ•´é™¤ num_headsï¼Œå¦å‰‡æœƒå ±éŒ¯ï¼


class sLSTMBlockConfig:
    '''
    å®šç¾©ä¸€å€‹åç‚º sLSTMBlockConfig çš„è¨­å®šé¡åˆ¥ï¼ˆconfiguration classï¼‰ï¼Œ
    é¡ä¼¼æ–¼ä¸€ç¨®â€œåƒæ•¸å°è£å™¨â€ï¼Œæ–¹ä¾¿æ¨¡å‹å…¶ä»–åœ°æ–¹å¼•ç”¨å’Œå‚³éè¨­å®šå€¼ã€‚
    '''
    def __init__(self, num_heads=8):  # å‡è¨­é è¨­ num_heads=8
        self.num_heads = num_heads
        self.slstm = self  # ç¢ºä¿slstmå±¬æ€§å¯å­˜å–
        # print(f"ğŸ” Debug: sLSTMBlockConfig - num_heads={self.num_heads}")
    def __post_init__(self):
        """é€™å€‹æ–¹æ³•æ˜¯ `xLSTMBlockStackConfig` æœŸæœ›èª¿ç”¨çš„ `__post_init__()` æ–¹æ³•"""
        # print(f"ğŸ”§ Debug: `sLSTMBlockConfig.__post_init__()` è¢«èª¿ç”¨")


class xLSTMBlockStack(nn.Module): 
    '''
    æ¨¡å‹çš„å †ç–Šå–®å…ƒï¼Œç®¡ç†å¤šå€‹sLSTMBlockã€‚
    è² è²¬å»ºç«‹ä¸€å±¤ä¸€å±¤å †ç–Šçš„ sLSTMBlock æ¨¡å‹ï¼ˆå³å¤šå±¤ LSTM çµæ§‹ï¼‰ã€‚
    : config.num_blocks = å¹¾å±¤ sLSTMBlock
    : config.slstm = æ¯å±¤çš„è¨­å®šï¼ˆsLSTMBlockConfigï¼‰
    '''
    def __init__(self, config: xLSTMBlockStackConfig):
        '''
        : config.num_blocks æœƒæ˜¯å¸Œæœ›å †å¹¾å±¤ LSTM blockã€‚
        : config.slstm_block æ˜¯æ¯ä¸€å±¤ block çš„è¨­å®šï¼ˆé€šå¸¸æ˜¯ sLSTMBlockConfig çš„å¯¦ä¾‹ï¼‰ã€‚
        '''
        super(xLSTMBlockStack, self).__init__()
        self.blocks = self._create_blocks(config=config) # è®€å– config.slstm_at(ä¹Ÿå°±æ˜¯index)ï¼Œé€å±¤åˆ¤æ–·è¦æ”¾å“ªå€‹ blockã€‚
        # print(f"ğŸ” Debug: åœ¨ xLSTMBlockStack å…§: embedding_dim={config.embedding_dim}")

    def _create_blocks(self, config):
        '''
        æ¯ä¸€å±¤ blockï¼ˆLSTM or sLSTMï¼‰æ˜¯æ€éº¼è¢«æ±ºå®šå’Œå»ºç«‹çš„ï¼Œ
        æœ€å¾ŒæŠŠæ‰€æœ‰ block ä¸²æˆä¸€å€‹æ¨¡å‹å †ç–Šçµæ§‹ã€‚
        : config.num_blocks æ§åˆ¶è¦å»ºç«‹å¹¾å±¤ blockã€‚
        '''
        blocks = []
        for i in range(config.num_blocks): # -- ç›®å‰åªæœ‰ sLSTMBlockï¼Œé è¨­æ˜¯éƒ½ä½¿ç”¨å®ƒï¼Œæ¯å±¤éƒ½ç”¨åŒä¸€ä»½è¨­å®šã€‚
            if i in config.slstm_at: 
                # config.slstm_at =>ã€ŒæŒ‡æ´¾å“ªå¹¾å±¤ç”¨ sLSTMã€ã€‚
                # _create_blocks() è®€å–è¨­å®šï¼Œæ±ºå®šè¦å †å“ªäº› Blockã€‚
                print(f"âœ… Block {i}: ä½¿ç”¨ sLSTMBlock")
                blocks.append(sLSTMBlock(config=config.slstm_block)) # å¯ä»¥å‰µå»ºå¤šå€‹sLSTMBlockã€‚
            else:
                print(f"ğŸ§± Block {i}: ä½¿ç”¨é è¨­ LSTMBlockï¼ˆæˆ–å…¶ä»–ï¼‰")
                print("!!! ç•¶å‰å°šæœªèª¿æ•´ !!! ç›®å‰ä»ç‚º sLSTMBlockï¼Œå¯è‡ªè¡Œæ›¿æ›ã€‚")
                blocks.append(sLSTMBlock(config=config.slstm_block))  # å¯ä»¥æ”¹æ”¾å…¶ä»– block é¡å‹ï¼Œä¾‹å¦‚ LSTMBlockã€‚
                # blocks.append(LSTMBlock(input_dim=config.embedding_dim)) # -- æ–°å¢ LSTMBlock é¡åˆ¥
        return nn.ModuleList(blocks) # ç¢ºä¿blocksæ˜¯nn.ModuleListã€‚ # ç”¨nn.ModuleListåŒ…è£èµ·ä¾†ï¼Œç”¨ä¾†å„²å­˜å¤šå€‹å­æ¨¡å‹ï¼Œç¢ºä¿å®ƒå€‘èƒ½è¢«è¨“ç·´ã€å­˜æª”ã€è½‰ç§»åˆ° GPUã€‚
        # ! ã€â†‘å•é¡Œã€‘é€™æ®µé›–ç„¶æœƒç–Šå¤šå±¤ blockï¼Œä½†å®ƒæ¯ä¸€å±¤éƒ½åªç”¨åŒä¸€å€‹ sLSTMBlockï¼Œæ²’æœ‰æ ¹æ“š slstm_at ä¾†åˆ¤æ–·æ˜¯å¦è©²ä½¿ç”¨ sLSTMBlock é‚„æ˜¯åˆ¥çš„ blockï¼ˆå¦‚ LSTMBlockï¼‰ã€‚
        # ! â†‘ é€™æ¨£å°±å¯ä»¥æ ¹æ“š slstm_at = [1] çš„è¨­å®šï¼Œåªåœ¨ç¬¬ 1 å±¤ä½¿ç”¨ sLSTMBlockï¼Œå…¶é¤˜ä½¿ç”¨å…¶ä»– blockï¼ˆç›®å‰ä»æ˜¯ sLSTMBlockï¼‰ã€‚
    
    def forward(self, x):
        """
        å‰å‘å‚³æ’­æ™‚é€å±¤é€šé blockï¼Œã€Œç–ŠåŠ ã€ç™¼ç”Ÿçš„åœ°æ–¹ã€‚
        :param x: (batch_size, sequence_length, embedding_dim)
        :return: (batch_size, sequence_length, embedding_dim)
        """
        for block in self.blocks: # blocks æ˜¯ list of block (ä¾‹å¦‚ï¼šblock0, block1, block2)
            x = block(x)  # æ¯å€‹ block æ¥æ”¶ä¸Šå±¤çš„è¼¸å‡ºï¼Œé€å±¤è¨ˆç®—ã€‚ \
                          # åŸ·è¡Œå®Œä¸€å±¤å¾Œï¼Œxæœƒæ›´æ–°ï¼Œå†ä¸Ÿé€²ä¸‹ä¸€å±¤ã€‚
        return x


class sLSTMBlock(nn.Module): 
    '''
    æ ¸å¿ƒè¨ˆç®—å–®å…ƒï¼Œæ•´å€‹xLSTMæ¶æ§‹ä¸­æœ€åº•å±¤çš„æ¨¡å‹é‹ç®—å–®ä½ï¼Œè² è²¬å°æ¯ä¸€æ®µè¼¸å…¥åºåˆ—åšLSTMè™•ç†ã€‚
    '''
    
    def __init__(self, config):
        '''
        åˆå§‹åŒ– sLSTMBlockã€‚
        : input_size=5 æ¯å€‹æ™‚é–“æ­¥çš„è¼¸å…¥ç‰¹å¾µæ•¸ï¼ˆå³ embedding ç¶­åº¦ï¼‰
        : hidden_size=5 LSTMçš„è¼¸å‡ºç¶­åº¦
        : num_layers=1 å–®å±¤LSTM
        : batch_first=True è¼¸å…¥æ ¼å¼ç‚º (batch_size, sequence_length, features)
        '''
        super(sLSTMBlock, self).__init__()
        self.num_heads = config.num_heads # sLSTMBlockä½¿ç”¨çš„ã€Œå¤šé ­æ•¸ã€ã€‚
        # é€™è£¡å¯ä»¥åŠ å…¥å…·é«”çš„ LSTM å±¤çµæ§‹
        self.lstm_layer = nn.LSTM(input_size=5, # è¼¸å…¥ç‰¹å¾µæ•¸
                                  hidden_size=5, # LSTM çš„è¼¸å‡ºç¶­åº¦
                                  num_layers=1, # å–®å±¤
                                  batch_first=True # è¼¸å…¥æ ¼å¼ç‚º (batch, seq, feature)
                                  )

    def forward(self, x):
        """
        å‰å‘å‚³æ’­
        :param x: (batch_size, sequence_length, embedding_dim)
        :return x: (batch_size, sequence_length, embedding_dim)
        :return _: (h_n, c_n)ï¼Œä¹Ÿå°±æ˜¯ LSTM çš„æœ€çµ‚ hidden state å’Œ cell stateï¼ˆé€™è£¡æ²’ç”¨åˆ°ï¼‰ã€‚
        """
        x, _ = self.lstm_layer(x)  # æŠŠåºåˆ— x ä¸Ÿé€² LSTMï¼Œè¼¸å‡ºæ¯å€‹æ™‚é–“æ­¥çš„ç‰¹å¾µï¼ˆä¸æ˜¯åªå–æœ€å¾Œä¸€æ­¥å–”ï¼‰ã€‚
        return x # å›å‚³ç¶“éLSTMè™•ç†å¾Œçš„çµæœï¼Œå°ä¸€æ®µåºåˆ—å¥—ç”¨LSTMè™•ç†çš„çµæœã€‚ # æ¯å€‹æ™‚é–“æ­¥éƒ½ç¶“éè®Šæ›ï¼Œä»£è¡¨æœ‰æ›´æ·±å±¤çš„æ™‚åºç†è§£
    

class LSTMBlock(nn.Module):
    def __init__(self, input_dim):
        super(LSTMBlock, self).__init__()
        self.lstm = nn.LSTM(input_size=input_dim, hidden_size=input_dim, batch_first=True)

    def forward(self, x):
        x, _ = self.lstm(x)
        return x


class xLSTMModel(nn.Module): # å‰µå»ºxLSTM
    def __init__(self, input_dim=5, output_dim=1, context_length=60, num_blocks=2, embedding_dim=5, use_slstm=False):
        """
        PyTorch ç‰ˆ xLSTM æ¨¡å‹ï¼Œå®šç¾©äº† xLSTMBlockStack ä½œç‚ºä¸»é«”ã€‚
        :param input_dim: è¼¸å…¥ç‰¹å¾µç¶­åº¦
        :param output_dim: è¼¸å‡ºç¶­åº¦
        :param context_length: è¨“ç·´æ™‚é•·
        :param num_blocks: xLSTM block æ•¸é‡
        :param embedding_dim: LSTM å…§éƒ¨ç‰¹å¾µç¶­åº¦
        :param use_slstm: æ˜¯å¦å•Ÿç”¨ sLSTM
        """
        super(xLSTMModel, self).__init__()

        slstm_config = sLSTMBlockConfig() if use_slstm else None # å¦‚æœ use_slstm = Trueï¼Œå°±æœƒå»ºç«‹ä¸€å€‹ sLSTMBlockConfig å¯¦ä¾‹ã€‚ # -- num_heads=8
        if slstm_config and hasattr(slstm_config, "num_heads"): # å¦‚æœå•Ÿç”¨äº† sLSTMï¼Œä¸¦ä¸”æœ‰num_headsï¼Œå‰‡æª¢æŸ¥ num_heads
            # hasattr(obj, "attr") => ç”¨ä¾†æª¢æŸ¥æŸå€‹ç‰©ä»¶ (obj) æ˜¯å¦ æœ‰æŸå€‹å±¬æ€§ (attr)ã€‚
            print(f"ğŸ” Debug: sLSTMBlockConfig.num_heads = {slstm_config.num_heads}")
            pass
        else:
            print(f"âš ï¸ Warning: sLSTMBlockConfig æ²’æœ‰ `num_heads` å±¬æ€§ï¼Œè«‹æª¢æŸ¥å…¶å®šç¾©ï¼")
            pass

        # å»ºç«‹ xLSTMBlockStackConfig
        self.xlstm_config = xLSTMBlockStackConfig(
            context_length=context_length,
            num_blocks=num_blocks, # æœ‰å¹¾å±¤ blockï¼ˆnum_blocksï¼‰
            embedding_dim=embedding_dim, # æ¯å±¤çš„è¼¸å…¥ç¶­åº¦æ˜¯å¤šå°‘ï¼ˆembedding_dimï¼‰ï¼Œè¨­å®šç‚º5ã€‚
            slstm_block=sLSTMBlockConfig() if use_slstm else None, # ç”¨ä»€éº¼æ¨£çš„ sLSTM é…ç½®(num_heads)
            slstm_at=[1] if use_slstm else [],  # å“ªå¹¾å±¤ä½¿ç”¨ sLSTMBlockã€‚é€™è£¡æ˜¯ index=1ï¼Œè®“ç¬¬äºŒå€‹ block ä½¿ç”¨ sLSTMã€‚ 
                                                # å¯ä»¥éˆæ´»é¸æ“‡å“ªå¹¾å±¤è¦ç”¨é€²éšçš„ sLSTMBlockï¼Œå…¶ä»–å±¤å°±ä¿ç•™ç‚ºæ™®é€šçš„ LSTMBlock æˆ–é è¨­ block çµæ§‹ã€‚
        )
        # print(f"ğŸ” Debug: åœ¨ xLSTMModel ä¸­: input_dim={input_dim}, embedding_dim={embedding_dim}, num_heads={num_heads}")

        self.xlstm_stack = xLSTMBlockStack(self.xlstm_config) # å°‡å‰›å‰›å»ºç«‹å¥½çš„ xLSTMBlockStackConfig é…ç½®ï¼Œå‚³å…¥ xLSTMBlockStack åšå¯¦ä¾‹åŒ–ï¼Œé€å±¤åˆ¤æ–·è¦æ”¾å“ªå€‹ blockï¼Œå»ºç«‹ä¸€å€‹ã€Œå¤šå±¤å †ç–Šçš„ xLSTM blockã€å †ç–Šé«”ã€‚
        self.batch_norm = nn.BatchNorm1d(embedding_dim) # åŠ å…¥BatchNormalizationæ‰¹æ¬¡æ­£è¦åŒ–ï¼Œå¹«åŠ©ç©©å®šè¨“ç·´ã€‚ # ! å› ç‚ºä½œç”¨åœ¨ (batch, features, time)ï¼Œæ‰€ä»¥è¦ permute() å…©æ¬¡ã€‚
        self.fc = nn.Linear(embedding_dim, output_dim) # å…¨é€£æ¥å±¤(fc)ï¼Œæœ€å¾Œåªå– æœ€å¾Œä¸€å€‹æ™‚é–“æ­¥çš„è¼¸å‡º â†’ åšç·šæ€§è½‰æ› â†’ è¼¸å‡ºé æ¸¬å€¼ã€‚

    def forward(self, x):
        """
        å‰å‘å‚³æ’­
        :param x: è¼¸å…¥å¼µé‡ (batch_size, sequence_length, input_dim=5)
        :return: é æ¸¬è¼¸å‡º
        """
        x = self.xlstm_stack(x)  # è¼¸å…¥åºåˆ—é€²å…¥å †ç–Šçš„ blockã€‚
        # self.xlstm_stack(x) é€™ä¸€è¡Œæ˜¯æ¨¡å‹çš„ã€Œå †ç–Š feature extractorã€ã€‚
        # å®ƒæœƒæ ¹æ“šè¨­å®šï¼Œè‡ªå‹•æŠŠ x é€é€²å¥½å¹¾å±¤ LSTM æˆ– sLSTM block è£¡é¢ã€‚
        # æ¯å±¤æœƒæ›´æ–°ç‰¹å¾µï¼Œè®“æ¨¡å‹æœ‰æ›´æ·±å±¤æ¬¡çš„ç†è§£èƒ½åŠ›ã€‚

        # running_mean should contain 60 elements not 5
        # ä»¥ä¸‹ä¸‰è¡Œæ˜¯ç‚ºäº†æ­£ç¢ºä½¿ç”¨ BatchNorm1dï¼Œå®ƒè¦æ±‚ (batch, channels, seq_len) çš„è¼¸å…¥æ ¼å¼ã€‚
        x = x.permute(0, 2, 1)  # è®Šæˆ (batch_size, embedding_dim, sequence_length)
        x = self.batch_norm(x)   # é€éæ‰¹æ¬¡æ­£è¦åŒ–ï¼Œä½¿è¨“ç·´æ›´ç©©å®š
        x = x.permute(0, 2, 1)   # è®Šå› (batch_size, sequence_length, embedding_dim)
        x = self.fc(x[:, -1, :])  # å…¨é€£æ¥å±¤è¼¸å‡ºï¼Œå–æœ€å¾Œä¸€å€‹æ™‚é–“æ­¥çš„è¼¸å‡ºä¾†åšé æ¸¬ã€‚
        return x
    

class EarlyStopping:
    """ 
    ç›£æ¸¬é©—è­‰Lossï¼Œè‹¥é€£çºŒå¤šæ¬¡ç„¡æ”¹å–„å‰‡æå‰çµ‚æ­¢è¨“ç·´ï¼Œé˜²æ­¢æ¨¡å‹éæ“¬åˆæˆ–æµªè²»è³‡æºæŒçºŒè¨“ç·´æ²’åœ¨é€²æ­¥çš„æ¨¡å‹ã€‚
    ç•¶é©—è­‰æŒ‡æ¨™é€£çºŒå¹¾å€‹ epoch æ²’æœ‰ã€Œè¶³å¤ çš„æ”¹å–„ã€ï¼Œå°±æå‰ä¸­æ­¢è¨“ç·´ï¼Œä¸¦å›å¾©ã€Œæœ€ä½³æ¨¡å‹æ¬Šé‡ã€ã€‚
    """
    def __init__(self, patience=10, min_delta=0.0001, monitor="val_loss", verbose=True, restore_best_weights=True):
        """
        :param patience: ç¶“éå¤šå°‘å€‹epochå¾Œæ²’æœ‰æ”¹å–„
        :param min_delta: æœ€å°æ”¹å–„å¹…åº¦ã€‚å¦‚æœæŒ‡æ¨™ï¼ˆå¦‚ val_loss æˆ– val_rmseï¼‰çš„æ”¹å–„å¹…åº¦å°æ–¼ min_deltaï¼Œå‰‡ä¸è¦–ç‚ºæœ‰æ•ˆæ”¹å–„ã€‚
        :param monitor: ç›£æ¸¬çš„æŒ‡æ¨™ ('val_loss' æˆ– 'val_rmse')
        :param verbose: æ˜¯å¦æ‰“å°EarlyStoppingè¨Šæ¯
        :param restore_best_weights: æ˜¯å¦åœ¨ EarlyStoppingè§¸ç™¼æ™‚å›å¾©æœ€ä½³æ¨¡å‹æ¬Šé‡
        """
        self.patience = patience
        self.min_delta = min_delta # æœ€å°æ”¹å–„å¹…åº¦ã€‚å¿…é ˆé€²æ­¥å¤ å¤šï¼Œæ‰å€¼å¾—å»¶é•·è¨“ç·´ã€‚
        self.monitor = monitor
        self.verbose = verbose
        self.restore_best_weights = restore_best_weights
        self.best_score = float('inf') # è¨˜éŒ„æœ€ä½³æŒ‡æ¨™
        self.counter = 0 # æ²’é€²æ­¥çš„æ¬¡æ•¸è¨ˆæ•¸å™¨
        self.early_stop = False # æ˜¯å¦æå‰åœæ­¢
        self.best_model_state = None  # ç”¨æ–¼å­˜å„²æœ€ä½³æ¨¡å‹æ¬Šé‡

    def __call__(self, score, model):
        """
        :param model: ç›®å‰çš„ PyTorch æ¨¡å‹
        :param score: ç›£æ¸¬çš„æ•¸å€¼ï¼ˆå¯ä»¥æ˜¯ val_loss æˆ– val_rmseï¼‰
        # :param val_loss: é©—è­‰æå¤± (MSE)
        # :param val_rmse: é©—è­‰ RMSE (å¯é¸)
        """
        # TODO: Delete
        # if self.monitor == "val_loss":
        #     score = val_loss
        # elif self.monitor == "val_rmse":
        #     score = val_rmse
        # else:
        #     raise ValueError("monitor åƒæ•¸åªèƒ½æ˜¯ 'val_loss' æˆ– 'val_rmse'")

        # å¦‚æœç•¶å‰åˆ†æ•¸æ¯”ä¹‹å‰æœ€ä½³çš„é‚„è¦å¥½ï¼Œå‰‡æ›´æ–°æœ€ä½³åˆ†æ•¸ä¸¦é‡ç½®patienceè¨ˆæ•¸ã€‚
        if score < self.best_score - self.min_delta: # è€Œä¸”æ”¹å–„å¹…åº¦å¤§æ–¼ min_delta
            self.best_score  = score # æ›´æ–° best_score
            self.counter = 0  # é‡ç½® patience è¨ˆæ•¸ ï¼ˆå› ç‚ºæœ‰é€²æ­¥ï¼‰
            if self.restore_best_weights:
                self.best_model_state = model.state_dict()  # ä¿å­˜æœ€ä½³æ¨¡å‹æ¬Šé‡
        else: # æ²’æœ‰é€²æ­¥
            self.counter += 1 # æ²’æœ‰æ˜é¡¯é€²æ­¥å°±æŠŠ counter åŠ  1
            if self.verbose:
                print(f"â³ EarlyStopping patience: {self.counter}/{self.patience}")
            if self.counter >= self.patience: # å¦‚æœ counter ç´¯ç©è¶…éè€å¿ƒï¼ˆpatienceï¼‰æ¬¡æ•¸
                self.early_stop = True  # å•Ÿå‹• EarlyStopping
                print(f"â¹ Early stopping triggered after {self.patience} epochs of no improvement.") # è€å¿ƒè¨ˆæ•¸ï¼šæ¯æ¬¡æŒ‡æ¨™æ²’æœ‰æ”¹å–„æ™‚ï¼Œæ‰“å°ç•¶å‰çš„è€å¿ƒè¨ˆæ•¸ï¼Œæ–¹ä¾¿èª¿è©¦ã€‚
            if self.restore_best_weights and self.best_model_state: # å¦‚æœå•Ÿç”¨äº† restore_best_weightsï¼Œå‰‡å›å¾©æœ€ä½³æ¬Šé‡ã€‚
                print("ğŸ”„ Restoring best model weights...")
                model.load_state_dict(self.best_model_state) # å›å¾©æœ€ä½³æ¨¡å‹æ¬Šé‡ï¼šç¢ºä¿æœ€çµ‚çš„æ¨¡å‹ä¸æ˜¯ä¾†è‡ªéæ“¬åˆçš„ epochã€‚


# è‡ªè¨‚ RMSE å‡½æ•¸
def rmse(y_true, y_pred): # å› ç‚ºKerasä¸¦æœªå…§å»ºRMSEä½œç‚ºæŒ‡æ¨™ï¼Œéœ€è¦è‡ªè¡Œå®šç¾©ä¸€å€‹è‡ªè¨‚çš„RMSEæŒ‡æ¨™å‡½æ•¸ã€‚
    '''
    Root Mean Squared Error (RMSE)
    RMSE æ˜¯ mse çš„å¹³æ–¹æ ¹ï¼Œæ›´ç›´è§€åœ°è¡¨ç¤ºèª¤å·®ï¼Œèˆ‡å¯¦éš›æ•¸æ“šå–®ä½ä¸€è‡´ã€‚
    '''
    return torch.sqrt(torch.mean((y_pred - y_true) ** 2))


# TODO: è¨“ç·´æ¨¡å‹å‡½æ•¸
def train_model(model, train_loader, val_loader, num_epochs=200, learning_rate=1e-4, device="cuda", save_file_path=None, early_stop_patience=10, monitor="val_loss"):
    # out_dir=None,
    """
    ä½¿ç”¨PyTorchè¨“ç·´xLSTMæ¨¡å‹ï¼Œä¸¦è¨˜éŒ„Lossä»¥ä¾›ç¹ªè£½å­¸ç¿’æ›²ç·šï¼ŒåŒæ™‚è¨˜éŒ„æ¯å€‹epochçš„æŒ‡æ¨™è‡³CSVã€‚
    :param model: xLSTMModel
    :param train_loader: è¨“ç·´æ•¸æ“šåŠ è¼‰å™¨
    :param val_loader: é©—è­‰æ•¸æ“šåŠ è¼‰å™¨
    :param num_epochs: è¨“ç·´é€±æœŸ
    :param learning_rate: å­¸ç¿’ç‡
    :param device: é‹è¡Œè¨­å‚™ ('cuda' or 'cpu')
    """
    model.to(device) # å°‡æ¨¡å‹æ¬åˆ° GPU æˆ– CPU
    print(f'Model Device: {next(model.parameters()).device}')
    optimizer = optim.Adam(model.parameters(), lr=learning_rate) # å®šç¾© optimizer (é¸æ“‡ Adam æœ€ä½³åŒ–å™¨)ã€‚
    criterion = nn.MSELoss() # æå¤±å‡½æ•¸ï¼Œä½¿ç”¨MSEä½œç‚ºæå¤±å‡½æ•¸ã€‚

    # å­˜è¨“ç·´ç´€éŒ„èˆ‡æ—©åœæº–å‚™
    train_loss_list = []  # è¨˜éŒ„è¨“ç·´ Loss
    val_loss_list = []  # è¨˜éŒ„é©—è­‰ Loss

    # å­¸ç¿’ç‡èª¿æ•´ç­–ç•¥ (ç•¶é©—è­‰ loss é€£çºŒ6æ¬¡æ²’æœ‰æ”¹å–„ï¼Œå­¸ç¿’ç‡æ¸›å°‘) 
    scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=6, verbose=True, min_lr=1e-7) # ç•¶é©—è­‰é›† (val_loss) é€£çºŒ6æ¬¡ æ²’æœ‰æ”¹å–„æ™‚ï¼Œå­¸ç¿’ç‡æœƒç¸®å° 50% (factor=0.5)ã€‚
    early_stopping = EarlyStopping(patience=early_stop_patience, monitor=monitor, verbose=True) # è‹¥ val_loss æŒçºŒç„¡æ”¹å–„ â†’ çµ‚æ­¢è¨“ç·´
    
    # è¿½è¹¤æœ€ä½³æ¨¡å‹
    best_val_loss = float('inf') # è¨­å®šä¸€å€‹æ¥µå¤§å€¼ï¼ˆç„¡é™å¤§ï¼‰ä¾†åˆå§‹åŒ–ï¼Œé€™æ¨£ç¬¬ä¸€å€‹ epoch çš„ val_loss ä¸€å®šæœƒã€Œæ¯”å®ƒå°ã€ï¼Œå°±æœƒè¢«è¦–ç‚ºæœ€ä½³ã€‚
    best_model_state = None # ç”¨ä¾†å­˜æ”¾ã€Œæœ€ä½³æ¨¡å‹çš„åƒæ•¸æ¬Šé‡ã€ï¼ˆä¹Ÿå°±æ˜¯ model.state_dict()ï¼‰ã€‚

    log_file = os.path.join(save_file_path, "epoch_log.csv") # å»ºç«‹ CSV ç´€éŒ„æª”æ¡ˆï¼Œè¨˜éŒ„è¨“ç·´æ­·ç¨‹ã€‚
    log_columns =  ["epoch", "loss", "lr", "mae", "mse", "rmse", "val_loss", "val_mae", "val_mse", "val_rmse"]
    log_df = pd.DataFrame(columns=log_columns) # å»ºç«‹ä¸€å€‹ç©ºè¡¨æ ¼ï¼Œæº–å‚™æ¯å€‹ epoch çµæŸå¾Œå¯«å…¥ä¸€è¡Œè³‡æ–™ï¼ˆè¨“ç·´çµæœï¼‰ã€‚

    #  TODO: è¨“ç·´è¿´åœˆï¼ˆæ¯å€‹ epochï¼‰
    for epoch in range(num_epochs):
        # æ¸…æ¢¯åº¦ â†’ é æ¸¬ â†’ è¨ˆç®—Loss â†’ åå‘å‚³æ’­ â†’ æ›´æ–°åƒæ•¸
        model.train() # è¨­å®šç‚ºè¨“ç·´æ¨¡å¼
        train_loss = 0.0 # è¨“ç·´æå¤±ï¼Œä»£è¡¨æ¨¡å‹åœ¨ã€Œè¨“ç·´é›†ã€ä¸Šçš„èª¤å·®ï¼Œæ˜¯æ¨¡å‹æ¯å€‹ epoch åœ¨è¨“ç·´è³‡æ–™ä¸Šçš„æ•´é«”æå¤±å¹³å‡ã€‚
        mae_train = 0.0  #è¿½è¹¤MAE
        mse_train = 0.0 #è¿½è¹¤MSE

        for inputs, targets in train_loader: # å¾train_loaderè®€å…¥è³‡æ–™
            inputs, targets = inputs.to(device), targets.to(device)
            optimizer.zero_grad() # æ¸…é™¤æ¢¯åº¦
            outputs = model(inputs) # å‰å‘å‚³æ’­ã€‚æŠŠ inputs è³‡æ–™é€é€²æ¨¡å‹ model ä¸­ï¼Œä¸¦å¾—åˆ°æ¨¡å‹çš„è¼¸å‡º outputsã€‚
            loss = criterion(outputs, targets) # è¨ˆç®—æå¤±
            loss.backward() # åå‘å‚³æ’­ï¼ˆåˆ†æéŒ¯åœ¨å“ªï¼‰ã€‚å¾ loss é–‹å§‹ï¼Œåå‘å‚³éèª¤å·®ï¼Œè¨ˆç®—æ¯å€‹åƒæ•¸çš„æ¢¯åº¦ï¼ˆgradientï¼‰ã€‚
                            # æ¢¯åº¦ => å°±æ˜¯å¦‚æœè¦è®“éŒ¯èª¤è®Šå°ï¼Œé€™å€‹åƒæ•¸æ‡‰è©²å¾€å“ªå€‹æ–¹å‘èª¿æ•´ã€‚
            optimizer.step() # æ›´æ–°æ¬Šé‡ï¼ˆæ ¹æ“šåˆ†æçµæœä¾†æ›´æ–°æ¬Šé‡ï¼‰ã€‚
                             # ä¾ç…§ã€Œæ¢¯åº¦æ–¹å‘ã€èª¿æ•´åƒæ•¸ï¼Œã€Œå­¸ç¿’ â†’ ä¿®æ”¹åƒæ•¸ â†’ è®“æ¨¡å‹åœ¨ä¸‹ä¸€æ¬¡åšå¾—æ›´å¥½ï¼ã€ã€‚
            train_loss += loss.item()
            mse_train += loss.item() #è¨ˆç®—MSE
            mae_train += torch.abs(outputs - targets).mean().item()  #è¨ˆç®—MAE

        # å…¨éƒ¨æ‰¹æ¬¡çµæŸå¾Œï¼Œå¹³å‡ lossã€maeã€mse
        train_loss /= len(train_loader)
        mse_train /= len(train_loader)
        rmse_train = mse_train ** 0.5
        mae_train /= len(train_loader)

        # TODO: é©—è­‰ï¼Œè¨ˆç®— Validation Lossã€‚
        model.eval()  # åˆ‡æ›ç‚ºé©—è­‰æ¨¡å¼
        val_loss = 0.0 # é©—è­‰æå¤±ï¼Œä»£è¡¨æ¨¡å‹åœ¨ã€Œé©—è­‰é›†ã€ä¸Šçš„èª¤å·®ï¼Œæ˜¯æ¨¡å‹åœ¨çœ‹éè¨“ç·´è³‡æ–™ä¹‹å¾Œï¼Œé©—è­‰åœ¨æ²’çœ‹éçš„è³‡æ–™ä¸Šæ˜¯å¦æ³›åŒ–è‰¯å¥½ã€‚
        mse_val = 0.0 # è¿½è¹¤MSE
        mae_val = 0.0  # è¿½è¹¤MAE
        with torch.no_grad(): # åœæ­¢è‡ªå‹•è¨ˆç®—æ¢¯åº¦ï¼Œä¸æœƒå¤šèŠ±è³‡æºåšæ¢¯åº¦è¨ˆç®—ã€‚
            for inputs, targets in val_loader:
                inputs, targets = inputs.to(device), targets.to(device)
                outputs = model(inputs) # å‰å‘å‚³æ’­
                loss = criterion(outputs, targets) # è¨ˆç®—æå¤±
                val_loss += loss.item()
                mse_val += loss.item() # è¨ˆç®—MSE
                mae_val += torch.abs(outputs - targets).mean().item()  # è¨ˆç®—MAE
        # å¹³å‡ lossã€maeã€mse
        val_loss /= len(val_loader)
        mse_val /= len(val_loader)
        rmse_val = mse_val ** 0.5
        mae_val /= len(val_loader)

        # æ›´æ–°å­¸ç¿’ç‡
        scheduler.step(val_loss)

        # å„²å­˜æœ€ä½³æ¨¡å‹
        if val_loss < best_val_loss: # è¨˜éŒ„è¨“ç·´éç¨‹ä¸­æœ€å¥½çš„æ¨¡å‹ç‹€æ…‹ï¼Œç•¶æ¯ä¸€å€‹ epoch çµæŸæ™‚ï¼Œå¦‚æœæ–°çš„é©—è­‰æå¤±è®Šå¾—æ›´å°ï¼Œå°±æœƒæ›´æ–°ã€‚
            best_val_loss = val_loss
            best_model_state = model.state_dict()
        
        # TODO: EarlyStopping åœæ­¢è¨“ç·´
        score = val_loss if early_stopping.monitor == "val_loss" else rmse_val
        early_stopping(score, model)
        if early_stopping.early_stop:
            print(f"â¹ è¨“ç·´æå‰çµ‚æ­¢æ–¼ Epoch {epoch+1}")
            break

        # âœ… è¨˜éŒ„ Loss
        train_loss_list.append(train_loss)  # ä¿®æ­£ç‚º train_loss
        val_loss_list.append(val_loss)  # ç›´æ¥è¨˜éŒ„ val_loss
        current_lr = optimizer.param_groups[0]["lr"] # å–å¾—ç•¶å‰å­¸ç¿’ç‡
        # è¨˜éŒ„Epochè³‡æ–™åˆ°DataFrameã€‚æœƒæŠŠæ¯ä¸€è¼ªçš„çµæœå¯«å…¥epoch_log.csvï¼Œè¿½è¹¤ Lossã€RMSE çš„è®ŠåŒ–æ›²ç·š 
        epoch_data = pd.DataFrame({
            "epoch": [epoch+1], # ç•¶å‰è¨“ç·´é€±æœŸï¼ˆå¾ 1 é–‹å§‹ï¼‰
            "loss": [train_loss], # è¨“ç·´æå¤±ï¼ˆMSEï¼‰
            "lr": [current_lr], # å­¸ç¿’ç‡
            "mae": [mae_train], # è¨“ç·´çš„ MAEï¼ˆå¹³å‡çµ•å°èª¤å·®ï¼‰
            "mse": [mse_train], # è¨“ç·´ MSE
            "rmse": [rmse_train], # è¨“ç·´ RMSE
            "val_loss": [val_loss], # é©—è­‰æå¤±
            "val_mae": [mae_val], # é©—è­‰ MAE
            "val_mse": [mse_val], # é©—è­‰ MSE
            "val_rmse": [rmse_val] # é©—è­‰ RMSE
        })
        log_df = pd.concat([log_df, epoch_data], ignore_index=True) # é€™è¡ŒæœƒæŠŠå‰›å‰›çš„ epoch_dataï¼ˆä¸€è¡Œè³‡æ–™ï¼‰åŠ é€²æ•´å€‹è¡¨æ ¼ log_df ä¸­çš„æœ€å¾Œä¸€è¡Œã€‚
        log_df.to_csv(log_file, index=False) # å¯«å…¥CSVã€‚é€™æ˜¯ã€Œæ¯ä¸€è¼ªéƒ½å¯«ä¸€æ¬¡ã€ï¼Œæ‰€ä»¥å³ä½¿ä¸­é€”ä¸­æ–·ï¼Œæª”æ¡ˆè£¡ä¹Ÿæœƒä¿ç•™ç´€éŒ„ã€‚ 
        print(f"Epoch {epoch+1}/{num_epochs} - Train Loss: {train_loss:.6f} - Val Loss: {val_loss:.6f} - Val RMSE: {rmse_val:.6f}")
    
    # === æ¨¡å‹è¨“ç·´çµæŸ ===
    # è®“æ¨¡å‹å›åˆ°æœ€ä½³ç‹€æ…‹
    if early_stopping.restore_best_weights and best_model_state is not None:
        model.load_state_dict(best_model_state) # å›å¾©åˆ°é‚£å€‹ã€Œè¡¨ç¾æœ€å¥½çš„æ¨¡å‹ã€ã€‚
        print("è¨“ç·´å®Œæˆï¼Œå·²æ¢å¾©æœ€ä½³æ¨¡å‹ç‹€æ…‹")
    # å„²å­˜æœ€ä½³æ¨¡å‹æ¬Šé‡åˆ°æª”æ¡ˆ
    if save_file_path:
        best_model_path = os.path.join(save_file_path, "best_model.pt") # å„²å­˜ç‚º PyTorch æ ¼å¼ (.pt)
        torch.save(best_model_state, best_model_path)
        print(f"âœ… æœ€ä½³æ¨¡å‹æ¬Šé‡å·²å„²å­˜è‡³: {best_model_path}")

    return model, train_loss_list, val_loss_list, optimizer # å›å‚³æœ€çµ‚æ¨¡å‹


# TODO: å»ºç«‹æ¨¡å‹çµæ§‹
def build_model(input_shape: tuple, # æ¨¡å‹çš„è¼¸å…¥å½¢ç‹€(timesteps, features)
                gpu=True, # è‹¥ç‚ºTrueå‰‡ä½¿ç”¨CUDAåŠ é€Ÿçš„LSTMå±¤ï¼ˆCuDNNLSTMï¼‰
                pre_model=None, # è‹¥æœ‰å‚³å…¥é è¨“ç·´æ¨¡å‹ï¼Œå‰‡å¯ä»¥å¾ä¸­è¼‰å…¥æ¬Šé‡ã€‚
                freeze=False, # è‹¥ç‚ºTrueï¼Œæœƒå°‡éƒ¨åˆ†å±¤è¨­ç‚ºä¸å¯è¨“ç·´ï¼Œç”¨æ–¼é·ç§»å­¸ç¿’ã€‚
                noise=None, # è‹¥è¨­å®šæ­¤åƒæ•¸ï¼ŒæœƒåŠ å…¥ä¸€å±¤é«˜æ–¯å™ªè²å±¤ï¼Œæ¨¡æ“¬æ•¸æ“šè®Šç•°ã€‚
                verbose=True, # æ˜¯å¦å°å‡ºæ¨¡å‹æ¶æ§‹
                ):
    """
    å»ºç«‹ xLSTM æ¨¡å‹
    æ ¹æ“šåƒæ•¸ä¾†çµ„è£ã€è¨­å®šã€ç”šè‡³è¼‰å…¥é è¨“ç·´æ¨¡å‹ã€‚
    """
    # è¨­ç½® GPU
    # æœ‰ GPU å¯ç”¨ (torch.cuda.is_available()) ä¸” å…è¨±ä½¿ç”¨ (gpu=True) => device = "cuda"
    # å¦å‰‡ => device = "cpu"
    device = "cuda" if torch.cuda.is_available() and gpu else "cpu"
    print(f"é‹è¡Œè¨­å‚™: {device}")

    # åƒæ•¸
    input_dim = input_shape[1]  # ç‰¹å¾µæ•¸é‡ = 5
    output_dim = 1  # é æ¸¬è¼¸å‡ºç¶­åº¦
    sequence_length = input_shape[0]  # æ™‚åºé•·åº¦

     # å‰µå»º xLSTM æ¨¡å‹ (åˆå§‹åŒ–æ¨¡å‹)
    model = xLSTMModel(
        input_dim=input_dim,
        output_dim=output_dim,
        context_length=sequence_length,
        num_blocks=2,
        embedding_dim=input_dim, # ç‰¹å¾µç¶­åº¦ï¼Œè¨­ç‚º5ï¼Œä¸¦è®“embedding_dim = input_dimã€‚
        use_slstm=True, # æ˜¯å¦å•Ÿç”¨ sLSTM block
    ).to(device)

    # åŠ è¼‰é è¨“ç·´æ¬Šé‡ï¼ˆå¦‚æœæœ‰ï¼‰
    if pre_model: # å¯¦ç¾ Transfer Learning æˆ–çºŒè¨“çš„é—œéµï¼ŒæŠŠä¹‹å‰è¨“ç·´å¥½çš„æ¨¡å‹æ¬Šé‡è¼‰é€²ä¾†ä½¿ç”¨ã€‚
        model.load_state_dict(pre_model.state_dict())
        model.to(device)  # ç¢ºä¿æ¬Šé‡è¼‰å…¥å¾Œé‚„æ˜¯å°æ‡‰åˆ° device
        print("æˆåŠŸè¼‰å…¥é è¨“ç·´æ¨¡å‹æ¬Šé‡")

    # è¨­ç½®æ¬Šé‡æ˜¯å¦å¯è¨“ç·´
    if freeze: # å‡çµæ¬Šé‡
        for param in model.parameters():
            param.requires_grad = False # æ˜¯ PyTorch ä¸­æ§åˆ¶ã€Œé€™å€‹åƒæ•¸æ˜¯å¦åƒèˆ‡è¨“ç·´ã€çš„è¨­å®šã€‚
        print("æ‰€æœ‰å±¤å·²å‡çµï¼Œæ¨¡å‹å°‡ä¸æœƒæ›´æ–°æ¬Šé‡")

    # æ‰“å°æ¨¡å‹è³‡è¨Š
    if verbose: # é¡¯ç¤ºå®Œæ•´æ¨¡å‹çµæ§‹ã€æ¯å±¤è¼¸å‡ºå¤§å°ã€åƒæ•¸é‡
        summary(model, input_size=(1, sequence_length, input_dim), col_names=["input_size", "output_size", "num_params", "kernel_size", "mult_adds"]) # print(model)

    return model, device


    
    print(f'æ˜¯å¦åŠ å…¥å™ªè²: {noise}')
    if noise:
        noise_input = GaussianNoise(np.sqrt(noise))(input_layer) # åŠ å…¥é«˜æ–¯å™ªè²å±¤ï¼Œç”¨æ–¼æ¨¡æ“¬æ•¸æ“šçš„éš¨æ©Ÿè®Šç•°ã€‚å±¬æ–¼æ­£å‰‡åŒ–æŠ€è¡“ï¼Œè€Œéæ•¸æ“šæ“´å……ï¼ˆData Augmentationï¼‰ã€‚np.sqrt(noise) è¡¨ç¤ºå™ªè²çš„æ¨™æº–å·®ã€‚
        dense = TimeDistributed(
            Dense(
                10,
                kernel_regularizer=regularizers.l2(0.01), # æ­£å‰‡åŒ–ï¼Œæ¸›å°‘æ¨¡å‹çš„éåº¦æ“¬åˆã€‚
                kernel_initializer=initializers.glorot_uniform(seed=0), # ä½¿ç”¨ Glorot å‡å‹»åˆå§‹åŒ–æ–¹æ³•å°æ¬Šé‡é€²è¡Œåˆå§‹åŒ–ï¼Œæœ‰åŠ©æ–¼æé«˜æ¨¡å‹çš„æ”¶æ–‚é€Ÿåº¦å’Œç©©å®šæ€§ï¼Œä¸¦ä¸”æœ‰æ•ˆæ¸›å°‘æ¢¯åº¦æ¶ˆå¤±æˆ–æ¢¯åº¦çˆ†ç‚¸å•é¡Œã€‚
                bias_initializer=initializers.Zeros() # å°‡åç½®åˆå§‹åŒ–ç‚º 0ã€‚
            )
        )(noise_input)

    else:
        dense = TimeDistributed(
            Dense( 
                10, # è¼¸å‡ºå–®å…ƒæ•¸ç‚º10çš„å…¨é€£æ¥å±¤ã€‚
                kernel_regularizer=regularizers.l2(0.01), # æ­£å‰‡åŒ–ï¼Œæ¸›å°‘æ¨¡å‹çš„éåº¦æ“¬åˆã€‚
                kernel_initializer=initializers.glorot_uniform(seed=0), # ä½¿ç”¨ Glorot å‡å‹»åˆå§‹åŒ–æ–¹æ³•å°æ¬Šé‡é€²è¡Œåˆå§‹åŒ–ï¼Œæœ‰åŠ©æ–¼æé«˜æ¨¡å‹çš„æ”¶æ–‚é€Ÿåº¦å’Œç©©å®šæ€§ã€‚
                bias_initializer=initializers.Zeros() # å°‡åç½®åˆå§‹åŒ–ç‚º 0ã€‚
            )
        )(input_layer)

    lstm1 = LSTM(
        60,
        return_sequences=True,
        kernel_regularizer=regularizers.l2(0.01), # æ­£å‰‡åŒ–ï¼Œæ¸›å°‘æ¨¡å‹çš„éåº¦æ“¬åˆã€‚
        kernel_initializer=initializers.glorot_uniform(seed=0), # ä½¿ç”¨ Glorot å‡å‹»åˆå§‹åŒ–æ–¹æ³•å°æ¬Šé‡é€²è¡Œåˆå§‹åŒ–ï¼Œæœ‰åŠ©æ–¼æé«˜æ¨¡å‹çš„æ”¶æ–‚é€Ÿåº¦å’Œç©©å®šæ€§ã€‚
        recurrent_initializer=initializers.Orthogonal(seed=0), # å°‡ LSTM çš„éæ­¸æ¬Šé‡åˆå§‹åŒ–ç‚ºæ­£äº¤çŸ©é™£ï¼Œä»¥ä¿ƒé€²æ¢¯åº¦ç©©å®šã€‚
        bias_initializer=initializers.Zeros() # å°‡åç½®åˆå§‹åŒ–ç‚º 0ã€‚
    )(dense)
    lstm1 = BatchNormalization()(lstm1) # æ­£è¦åŒ–ï¼Œç©©å®šè¨“ç·´éç¨‹ã€åŠ é€Ÿæ”¶æ–‚ï¼Œä¸¦æé«˜æ¨¡å‹çš„æ³›åŒ–èƒ½åŠ›ã€‚

    lstm2 = LSTM(
        60,
        return_sequences=False,
        kernel_regularizer=regularizers.l2(0.01), # æ­£å‰‡åŒ–ï¼Œæ¸›å°‘æ¨¡å‹çš„éåº¦æ“¬åˆã€‚
        kernel_initializer=initializers.glorot_uniform(seed=0), # ä½¿ç”¨ Glorot å‡å‹»åˆå§‹åŒ–æ–¹æ³•å°æ¬Šé‡é€²è¡Œåˆå§‹åŒ–ï¼Œæœ‰åŠ©æ–¼æé«˜æ¨¡å‹çš„æ”¶æ–‚é€Ÿåº¦å’Œç©©å®šæ€§ã€‚
        recurrent_initializer=initializers.Orthogonal(seed=0), # å°‡ LSTM çš„éæ­¸æ¬Šé‡åˆå§‹åŒ–ç‚ºæ­£äº¤çŸ©é™£ï¼Œä»¥ä¿ƒé€²æ¢¯åº¦ç©©å®šã€‚
        bias_initializer=initializers.Zeros() # å°‡åç½®åˆå§‹åŒ–ç‚º 0ã€‚
    )(lstm1)
    lstm2 = BatchNormalization()(lstm2) # æ­£è¦åŒ–ï¼Œç©©å®šè¨“ç·´éç¨‹ã€åŠ é€Ÿæ”¶æ–‚ï¼Œä¸¦æé«˜æ¨¡å‹çš„æ³›åŒ–èƒ½åŠ›ã€‚

    output_layer = Dense(
        1,
        activation='sigmoid', # æ¿€æ´»å‡½æ•¸ç‚ºsigmoidï¼Œé©åˆè¼¸å‡ºä¸€å€‹ç¯„åœåœ¨0åˆ°1ä¹‹é–“çš„é æ¸¬çµæœã€‚
        kernel_regularizer=regularizers.l2(0.01), # æ­£å‰‡åŒ–ï¼Œæ¸›å°‘æ¨¡å‹çš„éåº¦æ“¬åˆã€‚
        kernel_initializer=initializers.glorot_uniform(seed=0), # ä½¿ç”¨ Glorot å‡å‹»åˆå§‹åŒ–æ–¹æ³•å°æ¬Šé‡é€²è¡Œåˆå§‹åŒ–ï¼Œæœ‰åŠ©æ–¼æé«˜æ¨¡å‹çš„æ”¶æ–‚é€Ÿåº¦å’Œç©©å®šæ€§ã€‚
        bias_initializer=initializers.Zeros() # å°‡åç½®åˆå§‹åŒ–ç‚º 0ã€‚
    )(lstm2)

    model = Model(inputs=input_layer, outputs=output_layer) # å»ºç«‹æ¨¡å‹
    if savefig:
        plot_model(model, to_file=f'{write_result_out_dir}/architecture.png', show_shapes=True, show_layer_names=True) # ç¹ªè£½ç¥ç¶“ç¶²è·¯æ¨¡å‹çš„çµæ§‹ä¸¦å°‡å…¶ä¿å­˜ç‚ºåœ–ç‰‡æª”æ¡ˆ
    

    # transfer weights from pre-trained model (é·ç§»å­¸ç¿’ï¼šè¼‰å…¥é è¨“ç·´æ¨¡å‹æ¬Šé‡)
    if pre_model:
        for i in range(2, len(model.layers) - 1): #ï¼ˆè·³éè¼¸å…¥å±¤å’Œæœ€å¾Œè¼¸å‡ºå±¤ï¼‰           
            print(f"\n--- Layer {i}: {model.layers[i].name} ---")

            # ç²å–ç•¶å‰å±¤çš„åŸå§‹æ¬Šé‡
            original_weights = model.layers[i].get_weights() # æ¨¡å‹ç•¶å‰å±¤çš„åˆå§‹æ¬Šé‡ã€‚
            # print(f"Original Weights (Before):\n{original_weights}")
            
            # å°‡å°æ‡‰å±¤çš„æ¬Šé‡å¾é è¨“ç·´æ¨¡å‹è¼‰å…¥
            pre_trained_weights = pre_model.layers[i].get_weights() # å¾é è¨“ç·´æ¨¡å‹ä¸­åŠ è¼‰çš„æ¬Šé‡ã€‚
            # print(f"Pre-trained Weights (Loaded):\n{pre_trained_weights}")
            
            # ç²å–æ›´æ–°å¾Œçš„æ¬Šé‡            
            model.layers[i].set_weights(pre_model.layers[i].get_weights()) # å°‡å°æ‡‰å±¤çš„æ¬Šé‡å¾é è¨“ç·´æ¨¡å‹è¼‰å…¥ã€‚

            # ç²å–æ›´æ–°å¾Œçš„æ¬Šé‡
            updated_weights = model.layers[i].get_weights() # è¼‰å…¥é è¨“ç·´æ¬Šé‡å¾Œçš„æ¬Šé‡ã€‚
            # print(f"Updated Weights (After):\n{updated_weights}")

            # æª¢æŸ¥æ˜¯å¦èˆ‡é è¨“ç·´æ¨¡å‹ä¸€è‡´
            if all(np.array_equal(w1, w2) for w1, w2 in zip(updated_weights, pre_trained_weights)):
                print("Weights successfully updated to pre-trained weights!")
            else:
                print("Weights mismatch after update!")

            if freeze: # è‹¥ freeze=Trueï¼Œå‰‡å°‡é€™äº›å±¤è¨­ç½®ç‚ºä¸å¯è¨“ç·´ï¼ˆå³æ¬Šé‡ä¸æœƒåœ¨è¨“ç·´ä¸­æ›´æ–°ï¼‰ï¼Œé€™æ¨£å¯ä»¥ä¿æŒé è¨“ç·´æ¬Šé‡ä¸è®Šã€‚
                model.layers[i].trainable = False
                print(f"Layer {i} ({model.layers[i].name}) is now frozen and will not be updated during training.")
            else: # å¦å‰‡ï¼Œæ¬Šé‡å¯è¨“ç·´ã€‚å…è¨±æ¨¡å‹åœ¨æ–°æ•¸æ“šä¸Šå­¸ç¿’ç‰¹å®šæ¨¡å¼ã€‚
                model.layers[i].trainable = True # ä¿è­‰å±¤è¢«è¨­ç½®ç‚ºå¯è¨“ç·´ï¼ˆé˜²æ­¢ä¹‹å‰è¢«å‡çµï¼‰
                print(f"Layer {i} ({model.layers[i].name}) is trainable and its weights will be updated during training (fine-tuning).")

    # èª¿æ•´å„ªåŒ–å™¨&å­¸ç¿’ç‡ã€‚
    if pre_model:
        # å®šç¾©æ¯å€‹æ•¸æ“šé›†çš„å­¸ç¿’ç‡ã€‚é è¨“ç·´æ¨¡å‹çš„å¾®èª¿é€šå¸¸éœ€è¦æ›´å°çš„å­¸ç¿’ç‡ã€‚
        dataset_learning_rates = {
            'FishAquaponics_IoTpond2': 1e-5,  # é‡å° IoTpond2
            'FishAquaponics_IoTpond3': 1e-4,  # é‡å° IoTpond3
            'FishAquaponics_IoTpond4': 1e-4,  # é‡å° IoTpond4
        }
        current_dataset = write_result_out_dir.split(os.sep)[-1]  # æ ¹æ“šç³»çµ±è·¯å¾‘åˆ†éš”ç¬¦åˆ†å‰²è·¯å¾‘ï¼Œå–å¾—æœ€å¾Œä¸€å€‹è·¯å¾‘éƒ¨åˆ†ã€‚
        init_learning_rate = dataset_learning_rates.get(current_dataset, 1e-4)  # é»˜èªå­¸ç¿’ç‡ç‚º 1e-4
    else:
        # å…¶å®ƒ
        dataset_learning_rates = {
            'FishAquaponics_IoTpond3': 1e-5,  # é‡å° IoTpond3
        }
        current_dataset = write_result_out_dir.split(os.sep)[-1]  # æ ¹æ“šç³»çµ±è·¯å¾‘åˆ†éš”ç¬¦åˆ†å‰²è·¯å¾‘ï¼Œå–å¾—æœ€å¾Œä¸€å€‹è·¯å¾‘éƒ¨åˆ†ã€‚
        init_learning_rate = dataset_learning_rates.get(current_dataset, 1e-4)  # é»˜èªå­¸ç¿’ç‡ç‚º 1e-4ï¼Œé©åˆå¤§å¤šæ•¸æ¨¡å‹çš„åˆå§‹è¨“ç·´ã€‚
    print(f'åˆå§‹å­¸ç¿’ç‡: {init_learning_rate}')
    Adam_optimizer = Adam(learning_rate=init_learning_rate) # æ¨™æº–Adamå„ªåŒ–å™¨
    print(f'å„ªåŒ–å™¨åƒæ•¸: {Adam_optimizer.get_config()}')
    model.compile(optimizer=Adam_optimizer, loss='mse', metrics=['mse', rmse, 'mae','mape','msle']) # metricsæ˜¯æ¨¡å‹è¨“ç·´éç¨‹ä¸­ç”¨ä¾†ç›£æ§æ¨¡å‹æ€§èƒ½çš„æŒ‡æ¨™ã€è©•ä¼°æ¨¡å‹çš„è¨“ç·´æ•ˆæœã€‚ # ä½¿ç”¨å‹•æ…‹å­¸ç¿’ç‡èª¿æ•´ç­–ç•¥ï¼ˆå¦‚ ReduceLROnPlateauï¼‰ï¼Œè¨­å®šåˆå§‹å­¸ç¿’ç‡æœ‰åŠ©æ–¼æ›´å¥½åœ°æ§åˆ¶å­¸ç¿’ç‡ç¯„åœã€‚
    if verbose: print(model.summary())

    return model
    
'''
GaussianNoise é«˜æ–¯å™ªè²å±¤
å¥½è™•ï¼š
1.) é˜²æ­¢éæ“¬åˆï¼šæ¨¡å‹æœƒå­¸ç¿’åˆ°æ›´å¤šæ•¸æ“šçš„è®Šç•°æ€§ï¼Œè€Œä¸æ˜¯éåº¦æ“¬åˆè¨“ç·´æ•¸æ“šçš„ç‰¹å®šæ¨¡å¼ã€‚
2.) å¢åŠ æ³›åŒ–èƒ½åŠ›ï¼šæ¨¡å‹åœ¨è¨“ç·´æ™‚é‡åˆ°æ›´å¤šä¸åŒçš„æ•¸æ“šè¼¸å…¥ï¼Œå¾è€Œæé«˜æ¨¡å‹åœ¨æ¸¬è©¦æ•¸æ“šä¸Šçš„æ€§èƒ½ã€‚
3.) æ¨¡æ“¬æ•¸æ“šå™ªè²ï¼šå¹«åŠ©æ¨¡å‹åœ¨é¢å°çœŸå¯¦ä¸–ç•Œçš„å™ªè²æ•¸æ“šæ™‚è¡¨ç¾å¾—æ›´å¥½ã€‚

L2 æ­£å‰‡åŒ– 
ç›®çš„ï¼šæ¸›å°‘éåº¦æ“¬åˆï¼Œä¹Ÿå°±æ˜¯è®“æ¨¡å‹ä¸éæ–¼è²¼åˆè¨“ç·´æ•¸æ“šï¼Œä½¿å®ƒèƒ½å°æ–°æ•¸æ“šæœ‰æ›´å¥½çš„è¡¨ç¾ã€‚
åŸç†ï¼šL2 æ­£å‰‡åŒ–æœƒè®“æ¨¡å‹çš„æ¬Šé‡ï¼ˆå³æ¯å€‹è¼¸å…¥å°é æ¸¬çµæœçš„å½±éŸ¿ç¨‹åº¦ï¼‰ä¿æŒè¼ƒå°ï¼Œä»¥ä¾¿æ›´ç°¡å–®ã€æ›´å¹³æ»‘åœ°é©æ‡‰æ•¸æ“šã€‚
ä½œæ³•ï¼šL2 æ­£å‰‡åŒ–é€šéåœ¨æå¤±å‡½æ•¸ä¸­åŠ å…¥ä¸€é …èˆ‡æ¬Šé‡çš„å¹³æ–¹å’Œæˆæ¯”ä¾‹çš„æ‡²ç½°é …ä¾†æŠ‘åˆ¶æ¨¡å‹çš„è¤‡é›œåº¦ï¼Œå¾è€Œæ¸›å°‘éåº¦æ“¬åˆé¢¨éšªã€‚
å¦‚ä½•å¯¦ç¾ï¼šå®ƒæœƒæŠŠæ¯å€‹æ¬Šé‡å€¼çš„å¹³æ–¹ä¹˜ä¸Šä¸€å€‹å°ä¿‚æ•¸ï¼ˆå¦‚ 0.01ï¼‰ï¼Œç„¶å¾ŒæŠŠé€™äº›çµæœåŠ åˆ°æå¤±å‡½æ•¸ä¸­ã€‚é€™æœƒè®“æ¨¡å‹æ›´åå¥½å°æ¬Šé‡ï¼Œå¾è€Œæ¸›å°‘éåº¦æ“¬åˆçš„é¢¨éšªã€‚

Orthogonal åˆå§‹åŒ–
ç›®çš„ï¼šåˆå§‹åŒ–æ¨¡å‹æ¬Šé‡ï¼Œä½¿æ¨¡å‹åœ¨ä¸€é–‹å§‹è¨“ç·´æ™‚æ›´ç©©å®šã€‚
åŸç†ï¼šOrthogonal åˆå§‹åŒ–æœƒè®“æ¨¡å‹çš„åˆå§‹æ¬Šé‡å½¼æ­¤é–“ä¿æŒã€Œæ­£äº¤ã€ï¼Œä¹Ÿå°±æ˜¯èªªï¼Œå®ƒå€‘çš„æ–¹å‘å®Œå…¨ä¸ç›¸é—œã€‚é€™æ¨£èƒ½ç¢ºä¿è¨Šè™Ÿåœ¨å‚³ééç¨‹ä¸­ä¸æœƒéæ–¼æ¸›å¼±æˆ–è®Šå¾—éå¼·ï¼Œé€™å°æ–¼ RNN ç­‰æ™‚é–“åºåˆ—æ¨¡å‹ç‰¹åˆ¥æœ‰å¹«åŠ©ã€‚
å¦‚ä½•å¯¦ç¾ï¼šåœ¨æ¨¡å‹é–‹å§‹è¨“ç·´å‰ï¼Œç‚ºæ¬Šé‡åˆ†é…ä¸€çµ„ç‰¹æ®Šçš„åˆå§‹å€¼ï¼Œä½¿å®ƒå€‘çš„æ–¹å‘æ˜¯å½¼æ­¤ã€Œå‚ç›´ã€çš„ï¼ˆæ•¸å­¸ä¸Šç¨±ä¹‹ç‚ºã€Œæ­£äº¤çŸ©é™£ã€ï¼‰ã€‚

Glorot å‡å‹»åˆå§‹åŒ–æ–¹æ³• æ˜¯ä»€éº¼ï¼Ÿ
ç›®çš„ï¼šç¢ºä¿æ¨¡å‹ä¸€é–‹å§‹çš„æ¬Šé‡ä¸æœƒè®“æ•¸æ“šä¿¡è™Ÿåœ¨å‚³ééç¨‹ä¸­çˆ†ç‚¸æˆ–æ¶ˆå¤±ã€‚
åŸç†ï¼šGlorot å‡å‹»åˆå§‹åŒ–ï¼ˆä¹Ÿå« Xavier åˆå§‹åŒ–ï¼‰æœƒæ ¹æ“šè¼¸å…¥èˆ‡è¼¸å‡ºçš„ç¥ç¶“å…ƒæ•¸é‡ï¼Œé¸æ“‡ä¸€å€‹é©ç•¶çš„ç¯„åœï¼Œå¾ä¸­å‡å‹»åˆ†å¸ƒåœ°é¸å–æ¬Šé‡å€¼ã€‚é€™æ¨£å¯ä»¥ä¿æŒè¨Šè™Ÿç©©å®šï¼Œé¿å…æ¨¡å‹åœ¨è¨“ç·´åˆæœŸå°±å‡ºç¾æ¢¯åº¦çˆ†ç‚¸æˆ–æ¶ˆå¤±çš„å•é¡Œã€‚
å¦‚ä½•å¯¦ç¾ï¼šæ¨¡å‹çš„æ¬Šé‡æœƒéš¨æ©Ÿå¾ [-limit, limit] çš„ç¯„åœä¸­é¸å–ï¼Œå…¶ä¸­ limit æ˜¯åŸºæ–¼è©²å±¤çš„è¼¸å…¥å’Œè¼¸å‡ºå–®å…ƒæ•¸è¨ˆç®—å¾—ä¾†çš„ã€‚

BatchNormalization (æ‰¹æ¬¡æ­£è¦åŒ–) æ˜¯ä»€éº¼ï¼Ÿ
ç›®çš„ï¼šåŠ é€Ÿè¨“ç·´ï¼Œä¸¦è®“æ¨¡å‹æ›´ç©©å®šã€‚
åŸç†ï¼šæ‰¹æ¬¡æ­£è¦åŒ–æœƒåœ¨æ¯ä¸€å±¤çš„è¼¸å‡ºä¸Šé€²è¡Œã€Œæ¨™æº–åŒ–ã€ï¼Œä¹Ÿå°±æ˜¯æŠŠè¼¸å‡ºèª¿æ•´æˆä¸€å€‹æ›´ç©©å®šçš„ç¯„åœï¼ˆé€šå¸¸å¹³å‡å€¼ç‚º 0ï¼Œæ¨™æº–å·®ç‚º 1ï¼‰ã€‚é€™æ¨£åšå¯ä»¥è®“æ¨¡å‹æ›´å¿«æ‰¾åˆ°æœ€ä½³è§£ï¼Œæ¸›å°‘è¨“ç·´éç¨‹ä¸­çš„æ³¢å‹•ã€‚
å¦‚ä½•å¯¦ç¾ï¼šæ¨¡å‹æœƒæ ¹æ“šç•¶å‰çš„æ‰¹æ¬¡æ•¸æ“šï¼Œè¨ˆç®—å‡ºæ¯å±¤è¼¸å‡ºçš„å¹³å‡å€¼å’Œæ¨™æº–å·®ï¼Œç„¶å¾ŒæŠŠæ¯å€‹è¼¸å‡ºéƒ½èª¿æ•´åˆ°è©²ç¯„åœå…§ã€‚é€™æ¨£åšå¯ä»¥å¹³è¡¡è¼¸å‡ºçš„å¤§å°ï¼Œæå‡æ¨¡å‹çš„ç©©å®šæ€§å’Œæ”¶æ–‚é€Ÿåº¦ã€‚
'''
